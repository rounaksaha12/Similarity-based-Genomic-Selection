After clean(removal of rows with high missing values + redundancy removal) X.shape:  (397, 24108)
Fold 1, k_feat 8
X_train.shape:  (264, 8)
y_pred:  [ 0.79355418  0.73432597  0.79355418  0.79355418  0.79355418  0.79355418
  0.79355418  0.79355418  0.79355418  0.79355418  0.50766371  0.79355418
 -0.10699436 -0.83892302 -0.91598286  0.50766371  0.79355418  0.17663072
  0.13420563 -0.20221159 -0.5678325   0.79355418 -0.10076033 -0.21610183
 -0.91598286 -0.10699436  0.79355418 -0.09526366 -0.91598286  0.79355418
  0.79355418 -0.10699436  0.79355418  0.79355418  0.79355418 -0.10699436
 -0.5678325  -0.57650228 -0.91598286  0.39635771 -0.82031016 -0.74399336
 -0.63278491 -0.91598286 -0.91598286  0.58528824  0.79355418 -0.83059299
 -0.83892302 -0.91598286 -0.20845655 -0.83892302 -0.10076033 -0.42170845
 -0.10699436 -0.10699436 -0.10699436 -0.91598286 -0.83892302  0.79355418
 -0.20221159 -0.91598286 -0.83059299  0.73432597 -0.5678325  -0.17272485
 -0.5678325  -0.57570326 -0.5678325  -0.90326209  0.02868239 -0.21824004
  0.79355418  0.79355418  0.79355418 -0.75385937 -0.91598286 -0.07819786
 -0.3657146  -0.10699436 -0.10699436 -0.10699436 -0.10699436 -0.10699436
 -0.10699436  0.47902494 -0.10699436 -0.10699436 -0.07079981 -0.10699436
 -0.21824004 -0.10699436 -0.10699436 -0.20221159  0.79355418  0.79355418
 -0.91598286 -0.91598286  0.79355418  0.79355418  0.79355418 -0.91598286
  0.79355418 -0.07819786 -0.75385937  0.32809109  0.02868239 -0.583425
  0.13420563  0.73432597 -0.10076033 -0.57570326 -0.63278491 -0.40057648
 -0.57570326  0.02868239 -0.59757173  0.00535041 -0.47880671 -0.38043257
 -0.60727236 -0.81138333 -0.91598286 -0.41323347 -0.74098381 -0.83059299
 -0.83059299 -0.1596184   0.75570357 -0.60455334  0.02868239 -0.39662355
  0.61857106]
Pearson r: 0.632
MSE: 0.666
Fold 2, k_feat 8
X_train.shape:  (265, 8)
y_pred:  [ 0.74045301  0.74045301  0.74045301  0.74045301  0.74143863  0.74045301
 -0.27938948  0.3251716   0.3251716  -0.1395947   0.74045301  0.74045301
 -0.81678689  0.74045301 -0.81678689  0.32574972 -0.74085866 -0.16888816
 -0.81678689  0.2657128  -0.38270666  0.74045301  0.68278645  0.74045301
  0.74045301 -0.38036898 -0.25529647  0.30149959 -0.20414287 -0.69988747
 -0.38705043 -0.81678689  0.74045301 -0.2558249  -0.27949791 -0.27938948
 -0.27938948 -0.81678689  0.74045301  0.74045301  0.74045301  0.3251716
  0.6838202  -0.16888816  0.2657128  -0.27938948 -0.81678689 -0.81678689
 -0.38036898 -0.81678689 -0.38270666 -0.81678689 -0.81678689 -0.38036898
  0.74045301  0.74045301  0.74045301  0.2657128   0.74045301  0.2657128
  0.2427716   0.74045301  0.74045301  0.74045301  0.13250372  0.74045301
 -0.17058894  0.6838202   0.74045301 -0.17058894  0.3251716   0.13227535
  0.69496942  0.74045301  0.3251716  -0.23537736  0.74045301  0.74045301
  0.74045301  0.27100582 -0.74085866  0.74045301 -0.5177571  -0.16888816
 -0.22404231 -0.81678689 -0.81678689  0.3251716  -0.52571542  0.13250372
 -0.20514412  0.13250372  0.13250372  0.26513164  0.10650429  0.137843
  0.74045301  0.3251716   0.74045301  0.6838202   0.74045301  0.74045301
 -0.81678689 -0.7536681  -0.81678689 -0.81678689 -0.784077   -0.79887055
  0.74045301 -0.38270666  0.3251716  -0.67119483  0.6838202  -0.32550224
 -0.38705043 -0.27938948 -0.81674525  0.74143863  0.10650429 -0.6952023
 -0.21460779 -0.27129304 -0.81678689 -0.47628469 -0.40732587 -0.69519245
 -0.74992139 -0.2558249   0.74045301 -0.37130567 -0.37063047  0.26513164]
Pearson r: 0.618
MSE: 0.700
Fold 3, k_feat 8
X_train.shape:  (265, 8)
y_pred:  [ 0.60136607  0.60136607  0.60136607  0.60136607 -0.49311029  0.60136607
  0.29668223 -0.68739341 -0.72301705 -0.38693632  0.60136607  0.60136607
 -0.81126319 -0.0473925  -0.0473925  -0.0473925  -0.0473925  -0.9583446
  0.60136607  0.20935362 -0.03925584 -0.9583446   0.18482748 -0.71720276
  0.23441506 -0.9583446  -0.9583446  -0.58234878 -0.58234878  0.59220837
  0.60136607  0.60136607  0.60136607 -0.58234878  0.28742144 -0.9583446
  0.60136607 -0.9583446  -0.50348152  0.60136607 -0.49123248 -0.0473925
 -0.0473925  -0.72301705  0.60136607 -0.9583446   0.60136607  0.60136607
 -0.0473925   0.60136607 -0.03963552  0.20935362  0.60136607  0.60136607
  0.60136607 -0.00168448 -0.49123248  0.60136607  0.60136607 -0.49123248
  0.20935362 -0.58234878  0.60136607 -0.4808517  -0.73939458 -0.73939458
  0.28742144  0.56864399  0.59220837 -0.03963552  0.60136607  0.60136607
  0.54701458 -0.67736799 -0.9583446   0.23441506 -0.0473925   0.24334774
  0.23441506  0.22628751  0.23441506 -0.0473925   0.23441506 -0.01012642
  0.24334774 -0.0473925   0.28121292 -0.0473925   0.23441506  0.24875542
  0.28742144  0.23441506  0.27206783  0.60136607 -0.9583446  -0.9583446
 -0.9583446  -0.9583446   0.23441506  0.23441506  0.60136607  0.25481018
  0.60136607 -0.4808517   0.60136607 -0.90724132  0.48053998 -0.58234878
 -0.73205922 -0.78455073 -0.83247002 -0.65608407 -0.75099265 -0.80341729
  0.15749135 -0.72866508 -0.79965651 -0.64702333  0.27206783 -0.58442128
 -0.88341832 -0.7074045  -0.59172421 -0.37133757 -0.79193155 -0.75891926
 -0.94637851  0.13652803 -0.68739341 -0.67736799 -0.64451269 -0.94772373]
Pearson r: 0.475
MSE: 0.652
Average pearson_r accross all folds = 0.5750883646927597
Average mse accross all folds = 0.672572047945175
Fold 1, k_feat 16
X_train.shape:  (264, 16)
y_pred:  [ 0.7454395   0.73111424  0.7454395   0.7454395   0.7454395   0.7454395
  0.7454395   0.7454395   0.7454395   0.7454395   0.68765536  0.7454395
 -0.66481179 -0.80952671 -0.80710824  0.51162651  0.26123543  0.031574
 -0.21410108  0.26135783 -0.05699692  0.7454395  -0.49950949 -0.6183018
 -0.93217303 -0.66481179  0.7454395  -0.31115254 -0.93217303  0.11252428
  0.11252428 -0.66481179  0.7454395   0.11252428  0.7454395  -0.05301733
  0.02659839 -0.6979438  -0.80710824  0.05675144 -0.82521041 -0.79591479
 -0.74539922 -0.83883105 -0.83883105  0.08399741  0.7454395  -0.81336495
 -0.80916006 -0.60832002 -0.11836034 -0.82577191 -0.60672343 -0.71288289
  0.22333078  0.22333078 -0.66481179 -0.82566087 -0.80952671  0.7454395
  0.42991064 -0.80710824 -0.59884083  0.73111424  0.02659839  0.29608708
  0.02659839  0.03574567  0.02659839 -0.60115773  0.14624832 -0.61776693
  0.7454395   0.57816519  0.11252428 -0.5934257  -0.93217303 -0.6038884
 -0.0156945   0.22333078  0.22333078  0.22333078  0.22333078  0.23185058
  0.22333078 -0.21011524  0.22333078  0.22333078 -0.23408605  0.22333078
  0.25568332  0.22333078  0.22333078 -0.61496145  0.7454395   0.7454395
 -0.93217303 -0.93217303  0.7454395   0.7454395   0.7454395  -0.60832002
  0.7454395   0.29208147 -0.54463449 -0.56397616 -0.40154865 -0.82223316
 -0.57880295  0.73111424 -0.54595281 -0.72399005 -0.81898776 -0.63065126
 -0.44140283 -0.58518152 -0.50041511  0.29012217 -0.63211839 -0.53272212
 -0.49852787 -0.70771147 -0.80532995 -0.82352037 -0.53398367 -0.59884083
 -0.66214511 -0.51867595 -0.54038999 -0.53860071 -0.62250079 -0.5303744
 -0.5714782 ]
Pearson r: 0.707
MSE: 0.582
Fold 2, k_feat 16
X_train.shape:  (265, 16)
y_pred:  [ 7.94888859e-01  6.42353731e-01  7.94888859e-01  7.94888859e-01
  4.81085054e-01  7.94888859e-01 -5.87409652e-01  4.87662001e-01
  5.32024232e-01  3.65545891e-01  7.11168120e-01  7.11168120e-01
 -9.62593872e-01  7.94888859e-01 -9.62593872e-01  2.61876131e-01
 -7.43424738e-01  3.29412911e-01 -9.62593872e-01  2.36411334e-01
 -5.85720679e-01  7.94888859e-01  7.71220841e-01  7.94888859e-01
  7.94888859e-01 -3.00583994e-01  1.21338379e-02  5.18842534e-01
  1.77950356e-01 -4.71490160e-01 -2.89754052e-01 -9.62593872e-01
  7.94888859e-01 -3.50780614e-01 -5.12096555e-01 -4.63215775e-01
 -5.69989454e-01 -9.62593872e-01  7.94888859e-01  7.94888859e-01
  7.11168120e-01  5.32024232e-01  5.72156227e-01  3.29412911e-01
  3.41863356e-01 -5.30530935e-02 -9.62593872e-01 -9.62593872e-01
 -3.00583994e-01 -8.96073149e-01 -3.52860968e-01 -9.62593872e-01
 -9.62593872e-01 -3.00583994e-01  7.94888859e-01  7.94888859e-01
  7.94888859e-01  2.29503281e-01  7.94888859e-01  2.29503281e-01
  4.94948195e-01  7.94888859e-01  7.94888859e-01  7.11168120e-01
  1.62556272e-01  7.11168120e-01  1.24474106e-01  4.46224943e-01
  7.11168120e-01  1.24474106e-01  5.32024232e-01  1.88377520e-01
  6.02126707e-01  7.94888859e-01  5.03695724e-01 -3.19721322e-02
  7.94888859e-01  7.94888859e-01  7.94888859e-01  4.98825589e-01
 -8.09673024e-01  7.94888859e-01 -5.68203931e-01  3.29412911e-01
 -3.55262951e-01 -8.96073149e-01 -5.05399475e-01  5.03695724e-01
 -5.71842162e-01  1.62556272e-01 -4.70676697e-02  1.39063456e-01
  8.68034988e-02  3.47396860e-01  6.65763822e-04  1.64578688e-01
  7.94888859e-01  5.32024232e-01  7.94888859e-01  4.34331710e-01
  7.94888859e-01  7.94888859e-01 -9.62593872e-01 -4.11549906e-01
 -9.62593872e-01 -9.62593872e-01 -7.71848456e-01 -6.28893305e-01
  7.94888859e-01 -6.25761934e-01  5.32024232e-01 -7.85960152e-01
  5.72156227e-01 -4.35739737e-01 -3.59575684e-01 -5.87409652e-01
 -9.26997181e-01  6.23087327e-01  2.70787105e-02 -7.93279774e-01
 -4.79425776e-01 -5.17913895e-01 -8.70039847e-01 -3.92179324e-01
 -2.82603707e-01 -7.75150382e-01 -7.05674688e-01 -4.13092433e-01
  7.94888859e-01 -4.36291275e-01 -3.25001816e-01  3.41765292e-01]
Pearson r: 0.644
MSE: 0.662
Fold 3, k_feat 16
X_train.shape:  (265, 16)
y_pred:  [ 0.67483593  0.67483593  0.67483593  0.67483593 -0.33892806  0.67483593
  0.45336802 -0.27231951 -0.62372536 -0.48535413  0.67483593  0.67483593
 -0.854017   -0.0877097   0.07413977  0.00854283  0.07413977 -0.71492401
  0.67483593  0.3634339  -0.45320152 -0.95693292 -0.49010971 -0.74750592
 -0.38169671 -0.95693292 -0.72654802 -0.52391659 -0.52391659  0.67004345
  0.67483593  0.67483593  0.67483593 -0.52391659 -0.36391735 -0.72890953
  0.67483593 -0.95693292 -0.35902349  0.67483593 -0.65280155  0.00253715
  0.00253715 -0.55631971  0.67483593 -0.95693292  0.67483593  0.52749137
  0.00854283  0.67483593  0.07533868  0.22123422  0.67483593  0.67483593
  0.67483593  0.08242846 -0.64377828  0.67483593  0.67483593 -0.66542597
  0.22123422 -0.52391659  0.67483593 -0.32579522 -0.35188253 -0.35188253
 -0.36356064  0.63660914 -0.30395399  0.01490628  0.67483593  0.67483593
  0.50819236 -0.80708626 -0.95693292  0.38894062 -0.45489482 -0.37450863
  0.19030292  0.00732355  0.19030292  0.07413977  0.38894062 -0.53072912
  0.19824852 -0.41853349  0.4137883  -0.45489482  0.12988625  0.20072089
  0.43471286  0.19030292 -0.37207581  0.52749137 -0.95693292 -0.95693292
 -0.95693292 -0.95693292  0.40178507  0.19480974  0.67483593  0.40958785
  0.67483593 -0.33120535  0.67483593 -0.93933231  0.64347418 -0.52391659
 -0.41017046 -0.46550372 -0.60850115 -0.74238531 -0.3549685  -0.4910831
 -0.44913158 -0.81621038 -0.48818683 -0.53459626 -0.38333797 -0.76754485
 -0.65324319 -0.61757262 -0.36292249 -0.65544443 -0.83779675 -0.50509044
 -0.72256638 -0.34962159 -0.35925799 -0.74335657 -0.60973373 -0.95229584]
Pearson r: 0.574
MSE: 0.550
Average pearson_r accross all folds = 0.6414248072523209
Average mse accross all folds = 0.5981200780347901
Fold 1, k_feat 32
X_train.shape:  (264, 32)
y_pred:  [ 0.79882102  0.8174964   0.82358573  0.81742606  0.82358573  0.31435717
  0.31435717  0.14561799  0.31435717  0.82358573  0.51752103  0.56774975
 -0.62922455 -0.8536819  -0.88861256  0.43430961  0.52857305  0.299082
 -0.08615538  0.18455264 -0.15010081  0.31435717 -0.25324694 -0.55498285
 -0.97611646 -0.62922455  0.82358573 -0.06662828 -0.97611646  0.14135079
  0.14312419 -0.62922455  0.53952013  0.14761414  0.56098441 -0.0740521
  0.00896909 -0.69682463 -0.88861256 -0.04961879 -0.82094937 -0.76379201
 -0.72729117 -0.87911152 -0.88592639  0.12582654  0.82358573 -0.77456979
 -0.83923619 -0.66856844  0.16359885 -0.87900895 -0.53010434 -0.79233283
  0.14096374  0.14096374 -0.62922455 -0.86352638 -0.8536819   0.56774975
  0.33120928 -0.88861256 -0.58013156  0.8174964   0.00896909  0.18921229
  0.00896909  0.00364309  0.00896909 -0.58626072  0.11383812 -0.539973
  0.82358573  0.72390023  0.14135079 -0.56803522 -0.97611646 -0.52492385
  0.01593356  0.1450743   0.18595499  0.14096374  0.1450743   0.1486855
  0.16878235 -0.15084984  0.14096374  0.14096374 -0.10246157  0.14096374
  0.1877629   0.14096374  0.14096374 -0.48749008  0.81742606  0.79882102
 -0.97611646 -0.97611646  0.46510171  0.53809765  0.80484181 -0.66856844
  0.82358573  0.22166267 -0.60372094 -0.54996076 -0.20764814 -0.72456196
 -0.52789985  0.81135597 -0.488663   -0.67774052 -0.69121436 -0.63175932
 -0.41836123 -0.63840318 -0.55974421  0.43652436 -0.67033664 -0.43155951
 -0.60021974 -0.73208948 -0.86803116 -0.75837383 -0.55501646 -0.63858784
 -0.6851775  -0.53021453 -0.58438327 -0.61439451 -0.66919436 -0.5599116
 -0.52156948]
Pearson r: 0.695
MSE: 0.605
Fold 2, k_feat 32
X_train.shape:  (265, 32)
y_pred:  [ 0.86662642  0.77997235  0.86662642  0.86662642  0.73733815  0.86662642
 -0.13514546  0.73027259  0.02119498  0.5531688   0.87838367  0.87838367
 -1.00105471  0.86662642 -0.9357827  -0.3014442  -0.91726301  0.48326863
 -0.9357827   0.47081091 -0.73996789  0.17225744  0.85157323  0.86662642
  0.86662642 -0.55870061  0.30527044  0.59903725  0.38635925 -0.47119792
 -0.17932741 -0.7634885   0.82991297 -0.38935053 -0.51961309 -0.53259766
 -0.12991132 -0.9357827   0.86662642  0.86662642  0.87838367  0.68020781
  0.08354006  0.47722763  0.61230962  0.21255916 -1.00105471 -1.00105471
 -0.33664957 -0.560315   -0.48310876 -1.00105471 -0.9357827  -0.33664957
  0.86662642  0.86662642  0.86662642 -0.29366813  0.86662642  0.51737674
 -0.00130414  0.82991297  0.86662642  0.87838367  0.24340701  0.87838367
  0.25301441  0.59302078  0.87838367  0.27892255  0.64519486  0.29778455
  0.10674355  0.86662642  0.61401167  0.23812773  0.82991297  0.86662642
  0.86207883  0.3612678  -0.66183724  0.86662642 -0.64844316  0.48147568
 -0.30818557 -0.560315   -0.56756303  0.57193081 -0.70838121  0.25099978
  0.3172923   0.27451327  0.31691322 -0.29920951 -0.44799371  0.22153032
  0.86662642  0.68020781  0.86662642  0.65784559  0.86662642  0.16391213
 -0.9357827  -0.2372551  -0.9357827  -0.9357827  -0.65989246 -0.47837067
  0.86662642 -0.48093613  0.68020781 -0.77614482 -0.26872209 -0.40240555
 -0.60372836 -0.86128709 -0.98202419 -0.17596109 -0.44710941 -0.88869348
 -0.74097667 -0.6789739  -0.94711445 -0.55507578 -0.44721268 -0.87339403
 -0.57936328 -0.49689295  0.85621307 -0.53939039 -0.16145223  0.56427918]
Pearson r: 0.692
MSE: 0.591
Fold 3, k_feat 32
X_train.shape:  (265, 32)
y_pred:  [ 0.7204692   0.7204692   0.7204692   0.24907418 -0.31901094  0.7204692
  0.60456737 -0.20887077 -0.50072485 -0.50880055  0.7204692   0.71456713
 -0.78030362  0.11399428  0.21816557  0.2040742   0.21816557 -0.74002441
  0.7204692   0.46237664 -0.57540822 -0.9172312  -0.54941773 -0.50627654
 -0.51452284 -0.95852717 -0.86216259 -0.59470698 -0.21725437  0.17234472
  0.7204692   0.7204692   0.7204692  -0.59470698 -0.40430727 -0.83731251
  0.7204692  -0.9594892  -0.33967088  0.7204692  -0.60630636  0.1221993
  0.1221993  -0.20036483  0.7204692  -0.71733034  0.7204692   0.5493454
  0.2040742   0.7204692   0.20501665  0.39773659  0.7204692   0.7204692
  0.7204692  -0.1533609  -0.57121597  0.7204692   0.7204692  -0.60830937
  0.3964985  -0.48612565  0.7204692   0.05949006 -0.06326689 -0.06326689
 -0.19547046  0.16652184 -0.3804916   0.20902109  0.7204692   0.7204692
  0.43095572 -0.81789883 -0.95852717  0.46071343 -0.47951718 -0.23351623
  0.27843519  0.02085017  0.27843519  0.21816557  0.46071343 -0.46835159
  0.1982367  -0.29470981  0.57830768 -0.29743533  0.22884885  0.29300197
  0.11145972  0.27843519 -0.5481393   0.5493454  -0.95852717 -0.95852717
 -0.95852717 -0.95852717  0.07169283 -0.0389587   0.7204692   0.53745809
  0.7204692   0.05973843  0.7204692  -0.90282043  0.3641797  -0.59470698
 -0.14731305 -0.55662531 -0.37022054 -0.62098115 -0.25256744 -0.54947237
 -0.57998519 -0.89639008 -0.54869074 -0.44829868 -0.25074378 -0.67564787
 -0.56075067 -0.63153011 -0.09273528 -0.60186047 -0.89484198 -0.54972325
 -0.67078836 -0.31958038 -0.31711638 -0.63243549 -0.55537301 -0.93985425]
Pearson r: 0.614
MSE: 0.501
Average pearson_r accross all folds = 0.6670496703431049
Average mse accross all folds = 0.5657558279028713
Fold 1, k_feat 64
X_train.shape:  (264, 64)
y_pred:  [ 0.83391212  0.71482426  0.71526109  0.85980037  0.8632879   0.5239245
  0.5239245   0.417625    0.5239245   0.8632879   0.34267436  0.60797935
 -0.7579507  -0.81227928 -1.05390123  0.59949478  0.71735114  0.55965363
  0.01450739 -0.01135749  0.2157322   0.5239245  -0.43178703 -0.44680364
 -0.97567111 -0.7579507   0.8632879  -0.07696224 -1.03783098  0.38275741
  0.40000674 -0.72788474  0.58533079  0.37202217  0.72506797  0.04534225
  0.34403306 -0.53240884 -1.00592618  0.09379143 -0.68772954 -0.64203343
 -0.82568002 -0.96419951 -1.00825694  0.3023264   0.8632879  -0.6419073
 -0.84768734 -0.9083037   0.06969846 -0.72184284 -0.56137238 -0.59467511
  0.06329553  0.15004099 -0.72788474 -1.09201976 -0.78020102  0.70855859
  0.17414881 -1.00592618 -0.46771407  0.85964942  0.34403306 -0.16457928
  0.34430201  0.33053876  0.34403306 -0.44150856  0.07603049 -0.43669736
  0.8632879   0.74453281  0.38275741 -0.73499552 -1.00184068 -0.38401666
  0.12651426 -0.17189617  0.06767962  0.06014785  0.20436278 -0.00951051
  0.08152489 -0.23574244  0.2076809   0.21745392 -0.04262087  0.21745392
 -0.24070032  0.08570067  0.21745392 -0.39438584  0.85980037  0.83391212
 -1.03783098 -1.03783098  0.54188104  0.71471577  0.83732314 -0.58292627
  0.8632879   0.48810726 -0.68642383 -0.34791648 -0.16616018 -0.59903095
 -0.51391151  0.70382537 -0.46989184 -0.66895416 -0.65974276 -0.44910065
 -0.31107322 -0.48078702 -0.53097679  0.3705016  -0.8067426  -0.57729548
 -0.68852382 -0.71307206 -1.00454968 -0.70851961 -0.53452294 -0.54811406
 -0.8087095  -0.54578221 -0.62881286 -0.49867588 -0.62920351 -0.42557413
 -0.49698453]
Pearson r: 0.706
MSE: 0.576
Fold 2, k_feat 64
X_train.shape:  (265, 64)
y_pred:  [ 0.86681069  0.72819267  0.86681069  0.86681069  0.687289    0.86681069
 -0.34060617  0.3191247   0.36922675  0.52341533  0.82381009  0.82836003
 -1.04532903  0.86681069 -0.92183735 -0.17347218 -0.87533938  0.4754468
 -0.88874451  0.33630441 -0.65559068  0.48238575  0.73100023  0.86681069
  0.86681069 -0.41439538  0.02663359  0.48277719  0.36876012 -0.36317512
 -0.35173295 -0.83893816  0.84837595 -0.38130773 -0.53872257 -0.42235906
 -0.54460735 -0.92644464  0.86681069  0.86681069  0.82569278  0.75457202
  0.39466747  0.4727267   0.09624314 -0.01384352 -1.04532903 -1.04532903
 -0.30097883 -0.6325722  -0.44976274 -1.04532903 -0.90777227 -0.30097883
  0.71761066  0.86681069  0.86681069 -0.27661413  0.86681069 -0.04097956
  0.36545894  0.84837595  0.86681069  0.82569278  0.19773985  0.82836003
  0.38081933  0.38629082  0.82569278  0.44478425  0.66887948  0.07458572
  0.29675646  0.86681069  0.12301785 -0.30013564  0.84837595  0.86681069
  0.86393475  0.59212247 -0.67417922  0.84300189 -0.57894727  0.46977766
 -0.38179898 -0.63121147 -0.50077354  0.19896692 -0.63039118  0.19947882
  0.09813475  0.00415564  0.04273121 -0.30028035 -0.32099207  0.15510912
  0.86681069  0.75157162  0.86681069  0.11369134  0.86681069  0.48287232
 -0.90777227 -0.54097967 -0.90777227 -0.90777227 -0.62723403 -0.38940266
  0.86681069 -0.58882761  0.61767131 -0.87019036 -0.14052086 -0.44282806
 -0.54858168 -0.92647728 -1.02044157  0.06176813 -0.45355592 -0.8526195
 -0.75264698 -0.67522877 -0.87359613 -0.61718326 -0.5339819  -0.88003152
 -0.68203826 -0.51252116  0.4717945  -0.50155488 -0.40180467  0.08065966]
Pearson r: 0.706
MSE: 0.574
Fold 3, k_feat 64
X_train.shape:  (265, 64)
y_pred:  [ 7.77341879e-01  8.38867703e-01  8.38867703e-01  5.58133697e-01
  1.48497088e-01  7.49868944e-01  6.60102464e-01 -5.26982592e-01
 -6.63367003e-01 -4.50939860e-01  6.99837686e-01  7.54365812e-01
 -9.46864227e-01 -1.50076102e-04  1.42177628e-01  3.93006113e-02
  1.42177628e-01 -6.92091326e-01  7.77341879e-01  6.59916449e-01
 -7.00088143e-01 -6.72919032e-01 -3.77841709e-01 -3.98655166e-01
 -6.19275186e-01 -9.29055910e-01 -8.25517103e-01 -6.69683077e-01
 -5.26600001e-01  2.18288828e-01  6.17780322e-01  6.09729464e-01
  6.76237499e-01 -6.43302762e-01 -3.47662855e-01 -7.62583731e-01
  6.40558316e-01 -9.56971386e-01 -5.58555341e-01  6.76237499e-01
 -7.01574122e-01  2.46487817e-01  2.46487817e-01  2.54535390e-02
  7.87214789e-01 -5.24878117e-01  6.55590390e-01  7.21068043e-01
  2.41759262e-01  7.54048589e-01  2.15559740e-01  6.03576176e-01
  7.77341879e-01  4.05031817e-01  8.38867703e-01  5.84292750e-02
 -3.23034948e-01  6.20247263e-01  6.20247263e-01 -5.46270615e-01
  5.10281593e-01 -6.28331161e-01  8.14516357e-01  9.37691313e-02
 -5.00272146e-01  2.61179113e-02 -1.12151034e-01 -1.24711965e-03
 -6.18535568e-01  1.24666387e-01  8.38867703e-01  7.87214789e-01
  3.37257724e-01 -4.44366831e-01 -6.99088106e-01  1.55072011e-01
 -5.85517176e-01 -1.67313340e-01  1.62750837e-01  5.22637851e-02
  1.64306775e-01  1.42177628e-01  1.71318865e-01 -4.10602526e-01
  1.91604586e-01 -2.62624493e-01 -3.67599526e-01 -2.07399687e-01
  1.61782433e-01  1.57579150e-01  9.74523188e-02  1.62750837e-01
 -5.07424995e-01  6.34380473e-01 -9.87503437e-01 -9.29055910e-01
 -6.99088106e-01 -9.29055910e-01  1.00589690e-02  6.03000391e-02
  7.75046675e-01  6.84708682e-01  8.38867703e-01  2.80416593e-01
  7.16858492e-01 -9.63766426e-01  2.65117023e-01 -6.43302762e-01
 -5.22182159e-01 -2.06464406e-01 -5.56751110e-01 -7.26528273e-01
 -3.52060611e-01 -6.68256106e-01 -7.45282380e-01 -9.12718516e-01
 -6.09555932e-01 -7.80016690e-03 -5.24893871e-01 -6.10681831e-01
 -6.82319977e-01 -3.83028841e-01  1.02289441e-02 -5.03276615e-01
 -9.50684226e-01 -5.92977525e-01 -6.37847710e-01 -3.14969467e-01
 -5.13648964e-01 -7.67825282e-01 -2.20781751e-01 -6.67666533e-01]
Pearson r: 0.665
MSE: 0.450
Average pearson_r accross all folds = 0.6923359156005383
Average mse accross all folds = 0.5331533116681132
Fold 1, k_feat 128
X_train.shape:  (264, 128)
y_pred:  [ 0.87938692  0.77807826  0.80510185  0.89445     0.89632323  0.62783136
  0.62783136  0.56462044  0.62783136  0.89504021  0.39316568  0.63576152
 -0.75796403 -0.81965884 -1.11674026  0.6395926   0.58598262  0.40159921
 -0.020759   -0.08485097  0.1700449   0.62783136 -0.54348543 -0.60467851
 -1.01665464 -0.75493554  0.87776724 -0.14654773 -1.0424455   0.57768957
  0.58647395 -0.70803707  0.67414548  0.38047206  0.81753231  0.03011978
  0.20157781 -0.53907224 -1.04177156  0.16680197 -0.7483447  -0.6434848
 -0.77444317 -0.91671158 -0.93246978  0.40755521  0.89504021 -0.68357268
 -0.90460663 -1.01323519 -0.06374294 -0.67765584 -0.49334358 -0.64759811
  0.03914351  0.0485722  -0.70803707 -1.15355056 -0.78925326  0.80140587
  0.08750167 -1.09151943 -0.51944312  0.8508202   0.33643434 -0.1475299
  0.26206443  0.29278402  0.17580249 -0.51882113  0.01354634 -0.53252738
  0.89583115  0.76589309  0.57768957 -0.6780941  -1.02166244 -0.41042698
  0.10281461 -0.10883197 -0.08025369 -0.18258544  0.1108605  -0.05549094
  0.02355334 -0.31195499  0.11165651  0.14884783  0.0637162   0.15969585
 -0.32618752  0.02277526  0.115146   -0.43326741  0.89316726  0.79042633
 -1.0424455  -1.02983569  0.63180709  0.80446041  0.81577959 -0.59310836
  0.80597324  0.34672976 -0.57047225 -0.37191361 -0.11452061 -0.73327958
 -0.41225482  0.72099575 -0.41388325 -0.63689055 -0.66280383 -0.53225545
 -0.50148017 -0.48590571 -0.5806865   0.22918329 -0.8073239  -0.66510705
 -0.65542787 -0.72927318 -0.94320599 -0.67927802 -0.61468129 -0.7364784
 -0.79972672 -0.52369364 -0.64813424 -0.57977678 -0.69361407 -0.50312364
 -0.53209415]
Pearson r: 0.717
MSE: 0.557
Fold 2, k_feat 128
X_train.shape:  (265, 128)
y_pred:  [ 9.05178681e-01  7.63127410e-01  9.05178681e-01  9.22526895e-01
  6.41743756e-01  7.27043187e-01 -4.30042172e-01  5.79886729e-01
  5.78485873e-01  5.90765868e-01  8.12313621e-01  8.14220295e-01
 -1.12803203e+00  9.05178681e-01 -9.69447317e-01  1.80369488e-02
 -8.54586412e-01  2.47691187e-01 -9.90016701e-01  2.28408097e-01
 -6.35476691e-01  5.32384248e-01  8.38787552e-01  8.47524081e-01
  9.22526895e-01 -2.40419087e-01  5.09873241e-02  5.46986625e-01
  4.25542419e-01 -3.89078058e-01 -4.29834337e-01 -9.25870877e-01
  8.93760300e-01 -2.90062873e-01 -7.14106701e-01 -5.47822381e-01
 -8.12440684e-01 -9.22109116e-01  9.03926482e-01  9.21426491e-01
  8.59719546e-01  7.44588720e-01  2.50159316e-01  2.46544084e-01
 -1.68097804e-03 -1.33972334e-01 -1.13542492e+00 -1.13542492e+00
 -1.95382482e-01 -5.81029921e-01 -5.87821268e-01 -1.11062065e+00
 -1.00307838e+00 -1.95382482e-01  7.87288246e-01  9.08318282e-01
  7.98061707e-01 -1.65917261e-01  9.03926482e-01  9.20507599e-02
  5.04106477e-01  9.14298376e-01  9.22526895e-01  8.68408224e-01
  2.30905018e-01  8.69585134e-01  4.98393499e-01  2.15711991e-01
  8.59719546e-01  7.00943465e-01  4.99478529e-01 -2.62450448e-02
  4.35475595e-01  9.03926482e-01  1.87895775e-01 -2.01675644e-01
  7.66299528e-01  9.03926482e-01  9.03897741e-01  6.42336619e-01
 -6.20726981e-01  8.91288939e-01 -5.44109778e-01  2.23702893e-01
 -3.96995403e-01 -5.31992814e-01 -5.72126240e-01  3.94497880e-02
 -5.90678210e-01  2.38007117e-01  1.01460130e-01 -7.58358344e-02
  9.46488941e-04 -2.80291802e-01 -3.03297986e-01  2.01549073e-01
  9.21426491e-01  7.74864092e-01  7.87645883e-01  1.21115796e-01
  8.50516874e-01  6.65630873e-01 -1.00307838e+00 -6.32168238e-01
 -1.00307838e+00 -9.94694877e-01 -5.05278420e-01 -4.71722679e-01
  9.03926482e-01 -7.35506170e-01  7.86387125e-01 -8.48720154e-01
 -2.03547300e-01 -4.52555424e-01 -5.55504346e-01 -9.66389771e-01
 -9.36093982e-01  5.59573634e-02 -4.12140726e-01 -8.37706862e-01
 -7.62931130e-01 -7.06695621e-01 -9.12814155e-01 -4.56762606e-01
 -4.58385823e-01 -7.53645605e-01 -7.01562285e-01 -5.12707906e-01
  3.37491252e-01 -5.66289402e-01 -3.52351894e-01 -1.48318966e-02]
Pearson r: 0.707
MSE: 0.569
Fold 3, k_feat 128
X_train.shape:  (265, 128)
y_pred:  [ 0.85861467  0.65771318  0.87767188  0.74605493  0.05314096  0.63579737
  0.6976981  -0.62252836 -0.91918627 -0.41974412  0.6030746   0.8445517
 -0.98413269 -0.40941092  0.1554925  -0.40550007  0.1554925  -0.85249209
  0.59021717  0.55946786 -0.86619933 -0.82299483 -0.38000528 -0.29996679
 -0.64170474 -0.95645919 -0.91678612 -0.69752837 -0.60269112 -0.17013466
  0.78600991  0.77905919  0.79912214 -0.69656896 -0.26159129 -0.87110775
  0.77950431 -0.94422462 -0.3808483   0.5172008  -0.50755659  0.04134903
  0.04134903  0.03562713  0.84021377 -0.57536647  0.78190448  0.80709955
  0.08387073  0.65921232  0.21532266  0.44742569  0.67472676  0.21276154
  0.68262701  0.13119281 -0.33294895  0.53284826  0.53284826 -0.45101823
  0.71265593 -0.42499838  0.8703435   0.4488227  -0.31984583 -0.07164321
 -0.1541109   0.09223442 -0.45101563  0.15068736  0.80165753  0.5790938
  0.3031886  -0.4887904  -0.83215767  0.15782434 -0.46051652 -0.17905285
  0.17191403  0.09119255  0.17269132  0.13159263  0.16526127 -0.32086214
  0.23575627 -0.24860853 -0.06642166 -0.21774766  0.17060109  0.06799569
 -0.04832447  0.16057642 -0.37104035  0.28520116 -1.00743061 -0.98582632
 -0.8249163  -0.98119277 -0.10888553 -0.13520785  0.57786281  0.80098772
  0.78607522  0.5939986   0.64915059 -0.86971428  0.50565198 -0.69656896
 -0.37450215 -0.2351788  -0.77355761 -0.96685348 -0.55359321 -0.81433561
 -0.73994505 -0.86749782 -0.70126506 -0.2347339  -0.75962045 -0.67078509
 -0.74070176 -0.21178939  0.00764431 -0.47121214 -1.0577025  -0.65999561
 -0.62120974 -0.3709231  -0.44374417 -1.04215713 -0.25539363 -0.6738444 ]
Pearson r: 0.699
MSE: 0.419
Average pearson_r accross all folds = 0.7077392155030066
Average mse accross all folds = 0.5153471744611672
Fold 1, k_feat 256
X_train.shape:  (264, 256)
y_pred:  [ 0.78989227  0.73720039  0.78902318  0.82730162  0.82799027  0.61403499
  0.61290254  0.55553739  0.58152563  0.82694143  0.42879755  0.66535644
 -0.72027355 -0.73824957 -1.13037109  0.53774696  0.55145932  0.48571751
  0.15046687 -0.06104981  0.20331788  0.6127696  -0.44034821 -0.64684553
 -1.03519228 -0.78370635  0.79605689 -0.23576127 -1.0370255   0.64965898
  0.67173754 -0.73702852  0.70324352  0.57753123  0.79457414  0.08434638
  0.26803856 -0.42579304 -1.10185104  0.25616299 -0.65112981 -0.55007782
 -0.66191505 -0.78753046 -0.84474259  0.51671678  0.8187679  -0.68768861
 -0.86726127 -0.97779796 -0.06766889 -0.67961043 -0.58953855 -0.57727478
  0.15010123  0.13601139 -0.71258012 -1.12864912 -0.70394301  0.78660652
 -0.03707379 -1.12991791 -0.33664462  0.7785634   0.43455152 -0.05013847
  0.42040386  0.409618    0.255698   -0.56367967  0.14080234 -0.52119991
  0.82780999  0.67766282  0.64965898 -0.58100613 -1.00966711 -0.31848194
  0.10733749 -0.0453898  -0.04535536 -0.09491983  0.11888479  0.04260897
  0.05562959 -0.47888549  0.12565434  0.03175835  0.31874993  0.10885082
 -0.37439547  0.06007085  0.06419318 -0.34105835  0.81797107  0.74151324
 -1.01168467 -1.01170297  0.59644607  0.7413969   0.78764254 -0.70964383
  0.5601484   0.25607836 -0.58090783 -0.51567653 -0.18190204 -0.596759
 -0.53856592  0.45598492 -0.46286632 -0.6759243  -0.69582445 -0.53166544
 -0.47908683 -0.58817541 -0.58401707  0.23377284 -0.77383848 -0.61710765
 -0.61000056 -0.81029955 -0.92007576 -0.71874757 -0.61140881 -0.69212007
 -0.78174804 -0.59282181 -0.68356994 -0.48977119 -0.69699202 -0.50672055
 -0.64240725]
Pearson r: 0.706
MSE: 0.575
Fold 2, k_feat 256
X_train.shape:  (265, 256)
y_pred:  [ 0.83496666  0.31624548  0.69335328  0.77055266  0.28469381  0.53871208
 -0.49337038  0.6050351   0.54694259  0.55388763  0.55088428  0.55652491
 -1.24080291  0.87583901 -0.91818683 -0.15298846 -0.77920682  0.21541099
 -0.9387659   0.15592075 -0.54058247  0.53242423  0.83944111  0.79056301
  0.82740895 -0.37944159  0.19407907  0.33624089  0.53490456 -0.21060473
 -0.48632081 -0.89617081  0.86645797 -0.18636964 -0.66973668 -0.46622204
 -0.70971149 -0.92779113  0.8761285   0.89457136  0.73369758  0.747182
  0.33689714  0.22457851  0.05557042  0.01947899 -1.27333328 -1.27027749
 -0.40917287 -0.49321433 -0.63077993 -1.24099984 -0.96393157 -0.41198149
  0.73218404  0.84255895  0.81826871 -0.13787423  0.75452528  0.06742414
  0.37281648  0.80593313  0.85861466  0.83664102  0.2212108   0.80812139
  0.51252287  0.25951786  0.71356455  0.70493629  0.65034999  0.09730955
  0.44169206  0.79334639  0.06679338 -0.05922757  0.71615832  0.86498743
  0.85897723  0.48100245 -0.57932388  0.78352695 -0.45395851  0.20690369
 -0.56848929 -0.47191839 -0.59874808 -0.0216862  -0.59907338  0.2335155
  0.17700755  0.07318731  0.0443242  -0.22923742 -0.29590791  0.10035404
  0.79920932  0.76859679  0.77347213  0.04852288  0.84869283  0.71505811
 -0.95765903 -0.63552203 -0.93603777 -0.92261761 -0.45209284 -0.33893184
  0.76229733 -0.75582743  0.80172102 -0.82541935 -0.3169978  -0.39764103
 -0.70479424 -1.10546077 -0.97783625  0.08193503 -0.46030382 -0.75899808
 -0.82240888 -0.79331541 -0.96018807 -0.47799538 -0.63304633 -0.82171797
 -0.68169565 -0.6083983   0.24221594 -0.54473115 -0.48914997 -0.18497208]
Pearson r: 0.691
MSE: 0.597
Fold 3, k_feat 256
X_train.shape:  (265, 256)
y_pred:  [ 0.85173871  0.65968811  0.85580523  0.74108561  0.22291737  0.68726001
  0.70432625 -0.49311012 -0.87047805 -0.32505412  0.5819641   0.83151082
 -0.84796555 -0.42862728  0.18090845 -0.42769874  0.18090845 -0.86605569
  0.46579466  0.62873657 -0.78969182 -0.73910877 -0.38133815 -0.2986086
 -0.59121485 -0.95693501 -0.97991495 -0.6640243  -0.61163287 -0.1473742
  0.76463567  0.71985594  0.78354526 -0.60589831 -0.33404435 -0.83916125
  0.63393832 -0.87988676 -0.29905942  0.64010972 -0.58425259  0.08026228
  0.07308125  0.06656113  0.73207913 -0.53063985  0.77561983  0.80710341
  0.09827066  0.55023227  0.03320358  0.57038875  0.5694847   0.40992875
  0.70047311  0.05637618 -0.29965002  0.47331719  0.49048644 -0.40455881
  0.76109751 -0.4393951   0.86177563  0.37802377 -0.34362121 -0.14829785
 -0.22677659  0.08359773 -0.43052613 -0.01161703  0.74115971  0.4087544
  0.17987206 -0.50434855 -0.84415091  0.10275555 -0.42149028 -0.23209214
  0.16529563  0.07924566  0.16581146  0.17536758  0.15988357 -0.32399821
  0.10465224 -0.25126285 -0.05832679 -0.18539491  0.10360947  0.00708155
 -0.08797827  0.14911068 -0.40102276  0.38710185 -0.9671821  -0.96943298
 -0.81549294 -0.94545764 -0.00447394 -0.11724451  0.51973527  0.82857727
  0.7464308   0.69139284  0.68707812 -0.81084354  0.44212346 -0.60452552
 -0.34760963 -0.2998204  -0.68465259 -0.92326961 -0.5836492  -0.81166298
 -0.78799388 -0.86226886 -0.66930013 -0.19753959 -0.73365181 -0.67725252
 -0.72419698 -0.28539699 -0.06763388 -0.46447326 -1.0287357  -0.64179473
 -0.60317196 -0.3349399  -0.48862202 -0.85611506 -0.33367524 -0.62891648]
Pearson r: 0.706
MSE: 0.414
Average pearson_r accross all folds = 0.7011625853896394
Average mse accross all folds = 0.5284884550372292
Fold 1, k_feat 512
X_train.shape:  (264, 512)
y_pred:  [ 0.82121366  0.768267    0.82193301  0.82487161  0.81977238  0.64307204
  0.63610791  0.57877494  0.60143066  0.85724733  0.38883361  0.67787753
 -0.73086034 -0.61935515 -1.10758432  0.42095983  0.40391027  0.38058885
  0.15621972 -0.06491015  0.35330048  0.6394679  -0.46578084 -0.6218736
 -0.99837303 -0.7607583   0.81297736 -0.18087282 -0.99848783  0.74437192
  0.77406379 -0.67335459  0.72675842  0.61470752  0.8377551   0.00723399
  0.38493762 -0.25669084 -1.10113738  0.31426002 -0.48162736 -0.4731033
 -0.61861204 -0.70306007 -0.76635884  0.51539704  0.82472138 -0.58860242
 -0.74038607 -0.92365522 -0.21157263 -0.6000975  -0.62251    -0.58511493
  0.1930403   0.21195578 -0.68000779 -1.11263835 -0.60264928  0.81886903
 -0.03687348 -1.12083263 -0.38178826  0.80671472  0.51020731  0.05421631
  0.51581551  0.53484503  0.37851518 -0.46644977  0.19343082 -0.57798265
  0.83260025  0.69054597  0.74437192 -0.51529092 -0.96310786 -0.37804362
  0.08404232 -0.04599371 -0.04277992 -0.08451381  0.08412026  0.02876448
  0.04892608 -0.55183048  0.09241354  0.01881957  0.36678703  0.0900458
 -0.36868923  0.0538658  -0.00478418 -0.34781994  0.83183667  0.76381496
 -1.00078544 -0.94769575  0.57449596  0.78224549  0.79688769 -0.63203022
  0.5956277   0.29978485 -0.51106296 -0.55429857 -0.25604008 -0.55470936
 -0.50017406  0.51034565 -0.50379854 -0.58121303 -0.61141218 -0.51385819
 -0.40377114 -0.62545293 -0.51284086  0.17221948 -0.73262861 -0.44976205
 -0.5508635  -0.75695771 -0.89480008 -0.66476068 -0.53153322 -0.68827389
 -0.72852133 -0.59079722 -0.68182117 -0.50692745 -0.74572903 -0.4775719
 -0.61832676]
Pearson r: 0.696
MSE: 0.587
Fold 2, k_feat 512
X_train.shape:  (265, 512)
y_pred:  [ 0.76554892  0.43410698  0.77492501  0.7676723   0.33455948  0.61102902
 -0.54761177  0.66864028  0.40308879  0.65839058  0.51211349  0.51409521
 -1.27318017  0.81074075 -0.88548603 -0.1483477  -0.70655726  0.27186713
 -0.88743831 -0.08953453 -0.45617941  0.42166379  0.81946881  0.70451395
  0.75743584 -0.4624652   0.0878432   0.25838488  0.47686096 -0.18370935
 -0.55209805 -0.86644986  0.72030865 -0.26708205 -0.75281316 -0.48555287
 -0.64990113 -0.88247512  0.87643252  0.8796381   0.62542846  0.74271056
  0.10324779  0.20504006  0.03218423  0.09523483 -1.29021986 -1.28275202
 -0.37869051 -0.3198576  -0.62463771 -1.20465218 -0.87502394 -0.38046946
  0.68965958  0.79178615  0.77317756 -0.17067068  0.75747212 -0.04006464
  0.44336553  0.67478659  0.83481893  0.76715001  0.22202019  0.76563309
  0.48366554  0.05712624  0.61673125  0.59365771  0.71328406  0.0868435
  0.36523675  0.76780951  0.17269079 -0.10017181  0.73741592  0.83106815
  0.78298309  0.33045769 -0.54517527  0.70938008 -0.34712745  0.11846322
 -0.54183712 -0.41952477 -0.48451628  0.01013971 -0.47357399  0.23032757
  0.15707627  0.1019599  -0.02895426 -0.1793891  -0.26774183  0.11987258
  0.75303229  0.70242382  0.71275209 -0.00924588  0.82853093  0.61126226
 -0.89556887 -0.58760115 -0.86285738 -0.8716452  -0.42981127 -0.33919799
  0.76122672 -0.6197507   0.79397509 -0.77773475 -0.43567247 -0.41149296
 -0.68894528 -0.97674806 -0.97323331 -0.01688131 -0.5561804  -0.74233609
 -0.85838562 -0.77100374 -0.92712008 -0.60603634 -0.62555532 -0.87435616
 -0.66111859 -0.66280206  0.03961501 -0.57365261 -0.46405944 -0.2009278 ]
Pearson r: 0.709
MSE: 0.580
Fold 3, k_feat 512
X_train.shape:  (265, 512)
y_pred:  [ 0.82720811  0.54544451  0.81972602  0.69887429  0.19752147  0.72238905
  0.61202572 -0.52152593 -0.87315993 -0.25917869  0.66957472  0.80672455
 -0.7995348  -0.40454663  0.16106342 -0.40327545  0.16106342 -0.87712488
  0.48746968  0.54582955 -0.81929344 -0.69031828 -0.39922842 -0.21125729
 -0.40137778 -0.94921663 -1.06862109 -0.67262165 -0.63125418 -0.31017129
  0.75363879  0.71140953  0.78709323 -0.6057048  -0.28451929 -0.83784538
  0.61936442 -0.798509   -0.21376691  0.6407261  -0.51578048  0.02777776
  0.02601324  0.02078726  0.72164493 -0.53299264  0.71845939  0.80570819
  0.09761836  0.53942863 -0.06026646  0.62200044  0.47600682  0.40868334
  0.70691522  0.21843525 -0.19539442  0.55109564  0.55959236 -0.42419219
  0.76499963 -0.50385825  0.81900029  0.46762385 -0.34286321 -0.21953645
 -0.21324358 -0.08990639 -0.40853251 -0.10718585  0.71845908  0.50338613
  0.13197565 -0.38125499 -0.83422476  0.15478957 -0.28513124 -0.03134443
  0.14308541  0.13333909  0.13945099  0.14513579  0.16575258 -0.29704378
  0.15978398 -0.14138547  0.045908   -0.00518781  0.08641332 -0.07879194
 -0.10928546  0.15123324 -0.34991397  0.38129453 -0.90789181 -0.95871518
 -0.79442172 -0.93326102 -0.05096276 -0.14337387  0.54949912  0.7931037
  0.74193903  0.69578385  0.67350215 -0.82210302  0.49671826 -0.60794168
 -0.34781172 -0.39094146 -0.69311347 -0.9163079  -0.64737396 -0.85721871
 -0.82724191 -0.89271534 -0.76845434 -0.15716299 -0.74086955 -0.70165958
 -0.8151236  -0.39763253 -0.20943524 -0.43561249 -1.03430847 -0.6467879
 -0.61994196 -0.29518441 -0.61656993 -0.83031717 -0.47128811 -0.56172023]
Pearson r: 0.714
MSE: 0.406
Average pearson_r accross all folds = 0.7065262474893257
Average mse accross all folds = 0.5242930796997022
Fold 1, k_feat 1024
X_train.shape:  (264, 1024)
y_pred:  [ 0.77974869  0.71689713  0.7845078   0.8039458   0.8267233   0.59626517
  0.55942039  0.53330658  0.54224103  0.7865172   0.44806738  0.69614511
 -0.79776437 -0.66770227 -1.0770697   0.39702446  0.38041671  0.36641731
 -0.05850863 -0.08700661  0.32188143  0.56037878 -0.54612905 -0.56638341
 -0.98515763 -0.80633673  0.76933411 -0.18194161 -0.98717385  0.69215825
  0.71277695 -0.63734584  0.67868487  0.57796381  0.79285004 -0.02887336
  0.41272546 -0.18175162 -1.06888891  0.17307653 -0.41124614 -0.43008212
 -0.65178462 -0.72936578 -0.74929747  0.35703642  0.79029558 -0.55672508
 -0.73633497 -0.85203726 -0.18576497 -0.45869419 -0.57323919 -0.54690858
  0.13570437  0.13815704 -0.64591068 -1.06962263 -0.65642866  0.71567943
 -0.09184449 -1.07922572 -0.29160299  0.66332559  0.51106314  0.10969627
  0.60618136  0.51227197  0.43886686 -0.36539427  0.01276863 -0.54487286
  0.80121799  0.61595913  0.69373711 -0.45938892 -0.96488959 -0.39235757
  0.07352136 -0.03196301 -0.04192235 -0.0072131   0.07248954 -0.00363328
  0.04194056 -0.5874032   0.05383582  0.04964942  0.32873511  0.07199996
 -0.32898056  0.05325804 -0.0114011  -0.34429783  0.80477412  0.72774819
 -0.99494071 -0.95368045  0.63442817  0.77549085  0.75807269 -0.54562398
  0.62840615  0.20424263 -0.54618315 -0.61605745 -0.36070353 -0.47410867
 -0.56981017  0.31015158 -0.60098048 -0.59244008 -0.64177875 -0.57290396
 -0.36945422 -0.65075599 -0.55524027 -0.06628393 -0.68740402 -0.43384173
 -0.6509402  -0.68698179 -0.90196751 -0.66729372 -0.50546078 -0.69473887
 -0.6861838  -0.61420898 -0.64858591 -0.49816249 -0.72427587 -0.44869844
 -0.64256669]
Pearson r: 0.703
MSE: 0.585
Fold 2, k_feat 1024
X_train.shape:  (265, 1024)
y_pred:  [ 0.71298266  0.41532954  0.69253005  0.79871172  0.31922542  0.5767044
 -0.50267054  0.70023363  0.36188829  0.54728336  0.3660888   0.40491563
 -1.26141477  0.80922978 -0.84750352 -0.26304127 -0.68693049  0.18468264
 -0.85539105 -0.09494583 -0.43358617  0.24621984  0.8132926   0.68548365
  0.77939052 -0.58436566 -0.01846573  0.29472846  0.44265982 -0.12052488
 -0.50335065 -0.81094566  0.69548995 -0.29398491 -0.75605259 -0.48160254
 -0.68926268 -0.81948689  0.84376876  0.88514606  0.59584826  0.74385118
  0.01131303  0.16797213 -0.01156336  0.02588141 -1.29239271 -1.28197577
 -0.52186778 -0.21804445 -0.60177117 -1.17522029 -0.86617606 -0.52294319
  0.67295239  0.78548035  0.77195817 -0.17917583  0.78351615 -0.15547629
  0.50255484  0.65977733  0.78267311  0.72166707  0.19093108  0.70938719
  0.48601233  0.02390639  0.5906289   0.67449901  0.68175099  0.05538989
  0.34001252  0.72097021  0.15920441 -0.16866769  0.79663232  0.77612799
  0.74130569  0.34791668 -0.65167041  0.68303922 -0.36555239  0.07466044
 -0.53674583 -0.2593886  -0.51633498  0.06215214 -0.485584    0.21912204
  0.19415303  0.0315184  -0.02465621 -0.24330628 -0.23344919  0.15811724
  0.74283521  0.71298951  0.68076528 -0.0183534   0.81641588  0.67085358
 -0.88639629 -0.56021452 -0.85029679 -0.84830946 -0.41209406 -0.33676816
  0.78545365 -0.54815725  0.72128419 -0.82240286 -0.51521371 -0.34614424
 -0.73414015 -0.98750819 -0.98733913 -0.13885041 -0.62967066 -0.7168304
 -0.78283156 -0.81534856 -0.88985267 -0.6671312  -0.69807065 -0.79981771
 -0.6772921  -0.71568309  0.03585434 -0.56338916 -0.48114095 -0.3224324 ]
Pearson r: 0.703
MSE: 0.593
Fold 3, k_feat 1024
X_train.shape:  (265, 1024)
y_pred:  [ 0.8569242   0.57258888  0.83466303  0.73168479  0.26413841  0.79319356
  0.67954446 -0.49965006 -0.86470521 -0.22920967  0.74821401  0.7843449
 -0.74539295 -0.27708881  0.14142626 -0.27522477  0.13916087 -0.84095029
  0.3870708   0.56633458 -0.78813355 -0.66523725 -0.35518156 -0.13009924
 -0.25926432 -0.91742183 -1.15807439 -0.65184328 -0.61874454 -0.32215126
  0.74312723  0.68012901  0.75208392 -0.60153744 -0.25751655 -0.80711813
  0.58180061 -0.72626721 -0.11893664  0.64924573 -0.38765655  0.0464157
  0.04526075 -0.01360895  0.80817091 -0.54663406  0.68029939  0.76317821
  0.08648044  0.45751492 -0.08060494  0.58638217  0.48922132  0.42958357
  0.66713358  0.246158   -0.05219692  0.49736411  0.50178174 -0.40481331
  0.82684743 -0.49115825  0.88243081  0.54191945 -0.34565022 -0.19504194
 -0.22530021 -0.06310781 -0.33380424 -0.12151824  0.73808394  0.60407326
  0.00613744 -0.31585094 -0.80605266  0.12510598 -0.19707603 -0.03947596
  0.12930167  0.10938738  0.12634931  0.10346884  0.1238352  -0.27760548
  0.11086003 -0.11094021  0.14576826  0.02826488  0.05935742 -0.09456025
 -0.18686208  0.11889433 -0.36053035  0.46314431 -0.86912026 -0.91311986
 -0.7664106  -0.90315809 -0.06244529 -0.0709404   0.64661011  0.78410557
  0.78445416  0.74108897  0.58431438 -0.76929507  0.53852314 -0.60299165
 -0.24184146 -0.38915276 -0.71658552 -0.9211383  -0.61307569 -0.88317169
 -0.85569864 -0.90585449 -0.72636219 -0.16836848 -0.73550358 -0.74611876
 -0.79862722 -0.4990876  -0.19548789 -0.46310176 -1.03743904 -0.64315489
 -0.64779287 -0.34096531 -0.61585147 -0.85963015 -0.49755656 -0.63088449]
Pearson r: 0.717
MSE: 0.401
Average pearson_r accross all folds = 0.7077418906388887
Average mse accross all folds = 0.5262748829611887
Fold 1, k_feat 2048
X_train.shape:  (264, 2048)
y_pred:  [ 0.70036038  0.7009708   0.75899845  0.77222498  0.78394454  0.5466007
  0.51507612  0.50399357  0.51134777  0.68448485  0.40230167  0.61146563
 -0.78832723 -0.69493907 -1.06820234  0.34932178  0.37825353  0.41692505
 -0.16086686 -0.21416026  0.24675875  0.51665718 -0.5378283  -0.47909913
 -0.91176905 -0.82717342  0.64494173 -0.29306481 -0.90283422  0.63831132
  0.60985671 -0.63673342  0.57007698  0.51693852  0.72850326 -0.0607329
  0.38732729 -0.17487007 -1.06567656  0.07269454 -0.38598255 -0.45527623
 -0.68965037 -0.68814252 -0.70655478  0.2598472   0.71462109 -0.54451842
 -0.68370346 -0.82810277 -0.15594169 -0.38764129 -0.56295503 -0.51314439
  0.0502789   0.06381    -0.64248294 -1.05359881 -0.68724756  0.58551641
 -0.15677871 -1.06485352 -0.34427764  0.53568217  0.39672964  0.10688078
  0.62054627  0.46387817  0.42286167 -0.30305573 -0.08430602 -0.47155596
  0.72599453  0.50298252  0.63780263 -0.43549767 -0.90222627 -0.33594071
  0.05258157 -0.05447902 -0.05178369  0.01699583  0.05664234 -0.02780953
 -0.03427067 -0.58968502  0.05703287  0.02843689  0.22767344  0.05312568
 -0.3275631   0.06051593 -0.00523355 -0.35433503  0.75814475  0.65396948
 -0.90424308 -0.89444073  0.59643185  0.69561328  0.76219239 -0.53958333
  0.55057796  0.07753121 -0.60244363 -0.61310921 -0.33976986 -0.51074222
 -0.60842235  0.11019067 -0.64516038 -0.5816701  -0.61912048 -0.58411337
 -0.36406752 -0.65268709 -0.56859561 -0.12748185 -0.72218287 -0.35588902
 -0.66838045 -0.67628571 -0.89806237 -0.67028288 -0.55856874 -0.71054783
 -0.69113481 -0.65041289 -0.62207646 -0.50211252 -0.67593036 -0.45300786
 -0.61624269]
Pearson r: 0.711
MSE: 0.590
Fold 2, k_feat 2048
X_train.shape:  (265, 2048)
y_pred:  [ 0.63327257  0.36708157  0.55231237  0.72354974  0.31443621  0.5134922
 -0.42351093  0.64801669  0.33540762  0.47248392  0.26364366  0.25483918
 -1.22552059  0.69141387 -0.71843563 -0.36807688 -0.65195011  0.10244942
 -0.78888405 -0.10657363 -0.35900562  0.12440664  0.68186131  0.52276968
  0.64980443 -0.62682742 -0.02506606  0.28702373  0.29013948 -0.12520605
 -0.4489977  -0.72622313  0.60894081 -0.30253975 -0.78226844 -0.42179001
 -0.61314464 -0.73173612  0.80111197  0.80889209  0.3942069   0.6714828
 -0.04017066  0.10872963 -0.06822585  0.04577732 -1.25593085 -1.24882994
 -0.54954129 -0.11855338 -0.59135321 -1.17147472 -0.7891843  -0.55007646
  0.60070774  0.66799577  0.68538243 -0.18043927  0.74146981 -0.1678701
  0.46983955  0.49722564  0.68737675  0.60255568  0.1466278   0.57768183
  0.38936686 -0.03974735  0.39278077  0.60577157  0.63359731  0.00980167
  0.36930417  0.6646545   0.0573093  -0.14860891  0.6538905   0.72881488
  0.70091288  0.3068588  -0.65247024  0.48942699 -0.34120831 -0.03414279
 -0.54773334 -0.15506381 -0.51223862  0.01221795 -0.48081423  0.20088661
  0.17394607  0.00197154 -0.02571691 -0.25352635 -0.18049136  0.14448895
  0.65119137  0.67602327  0.62775565 -0.02150636  0.70920647  0.61130359
 -0.79399596 -0.52306888 -0.77210826 -0.77216188 -0.40699253 -0.37084301
  0.73721616 -0.49691797  0.70002523 -0.82781646 -0.5967326  -0.23358257
 -0.79833282 -0.97833442 -0.92712064 -0.27714525 -0.68918832 -0.68742217
 -0.71130816 -0.82502787 -0.86445662 -0.61059833 -0.69867103 -0.73999739
 -0.66514615 -0.69661147  0.03197941 -0.59750452 -0.50120957 -0.32124289]
Pearson r: 0.698
MSE: 0.618
Fold 3, k_feat 2048
X_train.shape:  (265, 2048)
y_pred:  [ 9.10029880e-01  6.76305273e-01  8.63114497e-01  7.21064301e-01
  2.58787596e-01  8.77883353e-01  7.16044650e-01 -4.53979248e-01
 -8.72101321e-01 -1.80720160e-01  8.12722930e-01  7.87821786e-01
 -7.09830758e-01 -2.36535259e-01  1.38965466e-01 -2.39074239e-01
  1.38703364e-01 -8.10074568e-01  3.35915475e-01  6.11742715e-01
 -7.89836584e-01 -6.36196485e-01 -3.18074408e-01 -1.25510643e-01
 -1.97007049e-01 -8.82525332e-01 -1.15832523e+00 -6.47892000e-01
 -6.33859180e-01 -3.48582013e-01  7.34760547e-01  6.42000104e-01
  7.58548054e-01 -5.90543271e-01 -2.20826165e-01 -7.37604694e-01
  5.49690474e-01 -6.35335286e-01 -1.94827406e-02  6.36937142e-01
 -3.15256577e-01  1.87999964e-02  2.12765306e-02 -2.60000239e-02
  8.66418256e-01 -5.75497006e-01  7.16538045e-01  7.94869828e-01
  1.82379763e-02  3.84985707e-01 -9.83068416e-02  5.50107243e-01
  4.32371974e-01  4.17218617e-01  6.20480428e-01  2.18275195e-01
 -1.66297302e-03  4.46671348e-01  4.48639558e-01 -3.78817277e-01
  8.86177147e-01 -5.15566076e-01  9.46005111e-01  6.06334515e-01
 -3.53203707e-01 -2.23068469e-01 -2.10342469e-01 -6.17462267e-02
 -2.98401404e-01 -1.44057024e-01  8.11552426e-01  6.27151477e-01
 -1.12666708e-01 -2.90418712e-01 -8.00330302e-01  1.12280034e-01
 -1.34429662e-01 -7.55242332e-02  1.19689346e-01  9.97941258e-02
  1.20031714e-01  1.13760876e-01  1.26136190e-01 -2.85238401e-01
  1.01946321e-01 -1.03294125e-01  2.03775073e-01 -9.17687251e-04
  4.36412024e-02 -1.18537609e-01 -2.18719992e-01  1.29895731e-01
 -3.51822212e-01  4.39608472e-01 -8.15691098e-01 -8.69381142e-01
 -7.50950212e-01 -8.81040554e-01 -8.89632783e-02 -8.45553430e-02
  6.71688427e-01  7.73003263e-01  8.24902473e-01  7.57535750e-01
  5.17985071e-01 -6.84844753e-01  5.68611904e-01 -5.92711580e-01
 -1.95113355e-01 -5.01043704e-01 -6.54147872e-01 -8.73578104e-01
 -5.64227735e-01 -8.46001730e-01 -8.59852421e-01 -9.13164688e-01
 -6.73045333e-01 -1.13647975e-01 -6.61007325e-01 -7.46052013e-01
 -7.68241140e-01 -5.64543116e-01 -1.36574090e-01 -4.62443254e-01
 -1.00823014e+00 -6.35132207e-01 -6.11877354e-01 -3.36728069e-01
 -5.87667497e-01 -8.14503516e-01 -5.24854052e-01 -6.21023265e-01]
Pearson r: 0.718
MSE: 0.399
Average pearson_r accross all folds = 0.7091408164649623
Average mse accross all folds = 0.5355334511940845
Fold 1, k_feat 4096
X_train.shape:  (264, 4096)
y_pred:  [ 0.66615408  0.62381261  0.69878392  0.77244155  0.79482527  0.50928291
  0.46382546  0.47133661  0.46356809  0.59053645  0.35342742  0.57654077
 -0.76706627 -0.69668994 -1.04210351  0.27511401  0.35936735  0.46520272
 -0.25110734 -0.3370996   0.21775228  0.46664012 -0.52024513 -0.36121093
 -0.83878301 -0.79386686  0.57392588 -0.39180425 -0.8389483   0.52136179
  0.48997452 -0.64239582  0.50045617  0.40285874  0.68610064 -0.06393615
  0.37095083 -0.11948583 -1.03832161 -0.02717179 -0.39782495 -0.47533995
 -0.72609661 -0.65519277 -0.67148735  0.16696741  0.67391599 -0.56980656
 -0.64908618 -0.77637074 -0.0835822  -0.33362956 -0.46881463 -0.46556199
 -0.03483335  0.02789199 -0.65688745 -1.0246186  -0.69416389  0.44648412
 -0.19643612 -1.03831305 -0.30252386  0.40923061  0.31746674  0.08805966
  0.60357904  0.47491386  0.40243815 -0.28348131 -0.13869275 -0.36538608
  0.70275217  0.41047325  0.52422549 -0.45318428 -0.82487576 -0.25216184
  0.07326357 -0.0061582  -0.03779953  0.03863035  0.07099514 -0.02861631
 -0.05629498 -0.54194071  0.06997325  0.03767658  0.0776233   0.05649349
 -0.40293597  0.05903714  0.02824378 -0.30725916  0.72114102  0.65387816
 -0.83373314 -0.82714202  0.57292031  0.71504058  0.74875021 -0.54216936
  0.51634453  0.01652835 -0.63566374 -0.60439997 -0.34815899 -0.51913537
 -0.59606902  0.02443954 -0.55514367 -0.50684952 -0.6237003  -0.59176857
 -0.3560107  -0.63492826 -0.59878761 -0.21972758 -0.69826242 -0.36469509
 -0.69238539 -0.60592617 -0.86946041 -0.66111121 -0.53158152 -0.73870375
 -0.62862772 -0.62027561 -0.55675878 -0.52051575 -0.61984918 -0.53500273
 -0.59590902]
Pearson r: 0.719
MSE: 0.595
Fold 2, k_feat 4096
X_train.shape:  (265, 4096)
y_pred:  [ 0.51150266  0.22538914  0.48720696  0.66851785  0.22579216  0.42061775
 -0.40411703  0.55419821  0.28902233  0.35412075  0.11991777  0.09874348
 -1.22002604  0.60885483 -0.58871779 -0.43696439 -0.61724634 -0.00263912
 -0.67190786 -0.07445417 -0.31200023  0.06137681  0.58036773  0.38687297
  0.59396353 -0.67512187 -0.07447474  0.2346826   0.18204005 -0.13417931
 -0.40698939 -0.6293025   0.53371397 -0.3183277  -0.80419555 -0.34890962
 -0.52435389 -0.62552219  0.74846237  0.74754608  0.25308624  0.58058328
 -0.07669697  0.04455988 -0.06348745  0.04935747 -1.24131549 -1.23475747
 -0.57915948 -0.06369831 -0.53075278 -1.17292485 -0.68143044 -0.57921919
  0.50604466  0.55224045  0.62539239 -0.17880775  0.68159932 -0.15162775
  0.40518518  0.41766293  0.61211216  0.45541303  0.15035479  0.45115899
  0.27156376 -0.05442473  0.25184214  0.55401023  0.53345595 -0.02889697
  0.30400641  0.62442466  0.01121598 -0.10385304  0.61091835  0.66797859
  0.60478124  0.22803862 -0.64734862  0.39885618 -0.3356209  -0.02729155
 -0.58501425 -0.08859369 -0.46826193 -0.00930139 -0.42124418  0.19430569
  0.14647418 -0.07200936 -0.05195542 -0.24249844 -0.18359286  0.14726236
  0.61288852  0.56950466  0.57467328 -0.04461134  0.66392074  0.56743053
 -0.67932998 -0.4810139  -0.66975161 -0.67043746 -0.35610259 -0.39006003
  0.68004255 -0.41116428  0.61798205 -0.81275397 -0.60678647 -0.23316836
 -0.83987779 -0.96816741 -0.90220739 -0.27916599 -0.6862556  -0.64120117
 -0.68213024 -0.80609339 -0.84692616 -0.62962118 -0.72105239 -0.69460588
 -0.66469677 -0.72279177  0.02418543 -0.62729119 -0.54272099 -0.38073808]
Pearson r: 0.687
MSE: 0.655
Fold 3, k_feat 4096
X_train.shape:  (265, 4096)
y_pred:  [ 8.44920878e-01  7.07387263e-01  7.79806229e-01  5.84701892e-01
  1.77511749e-01  8.43887491e-01  6.02194100e-01 -4.26703379e-01
 -8.77232627e-01 -6.64768779e-02  7.67639792e-01  8.00444464e-01
 -6.10776004e-01 -1.94008764e-01  1.22432566e-01 -1.93842206e-01
  1.22263829e-01 -7.61337027e-01  2.08647590e-01  5.44266217e-01
 -7.51318925e-01 -5.57690000e-01 -2.73653527e-01 -1.00714355e-01
 -1.34662874e-01 -8.47452891e-01 -1.16555431e+00 -6.25878002e-01
 -6.12090329e-01 -3.23926680e-01  7.11562754e-01  6.12911334e-01
  7.24897510e-01 -5.86310037e-01 -1.71884417e-01 -6.78960868e-01
  4.51096894e-01 -5.62061036e-01  3.48846623e-02  6.14492515e-01
 -2.28885545e-01 -6.97578815e-04  1.23871504e-03 -5.67427392e-02
  8.05625324e-01 -5.66576609e-01  7.54095488e-01  7.78485856e-01
  6.47120454e-03  2.78917249e-01 -1.16170246e-01  4.65603903e-01
  3.34857851e-01  3.35771091e-01  5.72468688e-01  1.93741089e-01
  5.64147342e-02  3.96586399e-01  3.96736694e-01 -3.30212659e-01
  8.20537165e-01 -5.17218223e-01  9.06132413e-01  5.41986899e-01
 -3.10587247e-01 -2.15287982e-01 -1.37355733e-01 -7.79928318e-02
 -2.53646610e-01 -1.00462450e-01  7.75092478e-01  5.41333294e-01
 -2.24008528e-01 -2.57977699e-01 -7.88857663e-01  1.10202915e-01
 -7.68874166e-02 -6.43717033e-02  1.18544691e-01  1.02233184e-01
  1.16807607e-01  1.23465473e-01  1.11271372e-01 -2.70251309e-01
  6.36148946e-02 -7.67119921e-02  2.47303249e-01  2.43452912e-02
  2.06705586e-02 -5.05765401e-02 -1.82074021e-01  1.20166720e-01
 -2.42961143e-01  3.64497696e-01 -7.44017536e-01 -8.16452762e-01
 -7.31164540e-01 -8.41594610e-01 -3.33140615e-02 -6.75522315e-02
  6.03542938e-01  7.07098868e-01  7.83315052e-01  6.67549474e-01
  4.44429935e-01 -5.70643441e-01  5.32186890e-01 -5.87406881e-01
 -1.72971891e-01 -5.34809658e-01 -6.32531429e-01 -8.79264786e-01
 -4.97731656e-01 -8.19879973e-01 -8.58468350e-01 -8.47297294e-01
 -6.39608931e-01 -1.92775795e-01 -6.86419412e-01 -7.44245416e-01
 -7.29870481e-01 -5.72155810e-01 -8.35587864e-02 -4.71450666e-01
 -9.97482378e-01 -6.50098436e-01 -5.92477968e-01 -3.61485825e-01
 -5.52802199e-01 -7.65728089e-01 -5.42284304e-01 -6.64839865e-01]
Pearson r: 0.709
MSE: 0.413
Average pearson_r accross all folds = 0.7051128689014584
Average mse accross all folds = 0.5541887744681423
Fold 1, k_feat 8192
X_train.shape:  (264, 8192)
y_pred:  [ 5.66646740e-01  5.03804951e-01  6.01786969e-01  6.85486899e-01
  7.41659846e-01  4.49923226e-01  3.99282905e-01  4.19117505e-01
  4.02866653e-01  4.76610585e-01  2.67294010e-01  3.99662871e-01
 -7.43725182e-01 -6.65703854e-01 -1.00710049e+00  1.40872849e-01
  3.41658917e-01  4.66846091e-01 -3.28794446e-01 -3.40756704e-01
  1.52038393e-01  4.00361701e-01 -5.03805288e-01 -2.98182938e-01
 -7.80848744e-01 -7.59422059e-01  4.58490631e-01 -4.16444647e-01
 -7.85734953e-01  3.81490583e-01  3.53084301e-01 -6.47333401e-01
  4.00024504e-01  2.79691076e-01  5.54088417e-01 -9.58885924e-02
  2.96396803e-01 -1.15836910e-01 -1.00090416e+00 -8.00649389e-02
 -3.63276424e-01 -4.62597125e-01 -6.78967109e-01 -5.81344436e-01
 -6.06080200e-01  9.79443497e-02  5.44307197e-01 -5.50024169e-01
 -5.89505707e-01 -7.16316337e-01 -5.47060871e-02 -2.70534330e-01
 -4.11864245e-01 -4.09649413e-01 -5.80055872e-02 -5.12581932e-03
 -6.56609071e-01 -9.86898376e-01 -6.66865817e-01  3.07786437e-01
 -2.21743059e-01 -1.00791872e+00 -2.53597176e-01  2.95631017e-01
  2.36253922e-01  7.28565144e-02  5.45318225e-01  3.56644545e-01
  3.18917682e-01 -2.72714029e-01 -2.08608001e-01 -3.01330701e-01
  6.39818646e-01  3.20941917e-01  3.82319939e-01 -4.77444142e-01
 -7.80887426e-01 -2.26576502e-01  7.15933773e-02 -8.59665932e-04
 -6.03428027e-02  4.84954231e-02  6.79253968e-02 -3.08318172e-02
 -8.39690671e-02 -4.72198225e-01  6.85970483e-02  3.55303925e-02
 -2.79774215e-02  5.98743202e-02 -3.88320462e-01  6.31117741e-02
  3.95826911e-02 -2.88413510e-01  5.90536472e-01  5.94408590e-01
 -7.79254884e-01 -7.75278229e-01  5.02323855e-01  6.36137087e-01
  7.13298370e-01 -5.39607617e-01  4.76467794e-01 -6.68273239e-02
 -6.44687228e-01 -5.92409699e-01 -4.04789309e-01 -4.77512655e-01
 -5.49541024e-01 -9.59857309e-02 -5.39040145e-01 -4.29437855e-01
 -5.70563067e-01 -5.77713896e-01 -3.19868246e-01 -5.99949889e-01
 -6.29896926e-01 -3.26437752e-01 -6.63207137e-01 -3.27178614e-01
 -6.79718849e-01 -5.08375958e-01 -8.40533224e-01 -6.22569101e-01
 -5.22948671e-01 -7.12651142e-01 -5.81042569e-01 -6.11810701e-01
 -5.08389676e-01 -5.14453620e-01 -5.92178864e-01 -5.54841355e-01
 -5.70228053e-01]
Pearson r: 0.726
MSE: 0.617
Fold 2, k_feat 8192
X_train.shape:  (265, 8192)
y_pred:  [ 0.37367522  0.13220229  0.36851727  0.57017061  0.15743253  0.27762936
 -0.41360793  0.43336774  0.21541481  0.16305617  0.00661453 -0.01460183
 -1.20542287  0.49290328 -0.46718366 -0.49123383 -0.58445431 -0.08765424
 -0.58115997 -0.16155274 -0.29256297 -0.04310264  0.47435436  0.22217007
  0.51065765 -0.66969269 -0.09194131  0.11329962  0.03651007 -0.2129453
 -0.37261554 -0.53607282  0.44491963 -0.37079045 -0.79087278 -0.32419869
 -0.45673054 -0.50056342  0.67927172  0.62686535  0.10876323  0.48890654
 -0.13503935 -0.0539093  -0.1131567   0.04339886 -1.22002525 -1.21948142
 -0.59611912 -0.05273331 -0.47427721 -1.14339037 -0.58734645 -0.59655844
  0.35300637  0.4031081   0.54169153 -0.20267664  0.5621951  -0.20631296
  0.26308087  0.27180131  0.42652974  0.25837451  0.11747493  0.27486035
  0.14706373 -0.12154228  0.10946893  0.49386239  0.45440793 -0.05712948
  0.18996575  0.54327524 -0.10157317 -0.15121849  0.5211333   0.57960801
  0.53764119  0.14241523 -0.63812452  0.27670377 -0.31403777 -0.14600475
 -0.58746825 -0.07169736 -0.42075851 -0.10889743 -0.406205    0.16347508
  0.13941256 -0.16092902 -0.06817862 -0.24471309 -0.20348687  0.1391647
  0.5195603   0.48393809  0.49378756 -0.11216757  0.55190268  0.45325018
 -0.58349843 -0.42548298 -0.58114207 -0.57914517 -0.38686845 -0.37120631
  0.56021071 -0.40265112  0.4933843  -0.75966148 -0.61668344 -0.23753417
 -0.82376509 -0.92218219 -0.86444095 -0.34263827 -0.67430949 -0.62202112
 -0.65680766 -0.7741527  -0.82095838 -0.62393709 -0.73981368 -0.69238691
 -0.64090343 -0.69541437 -0.0937739  -0.61114607 -0.5482987  -0.4022999 ]
Pearson r: 0.680
MSE: 0.702
Fold 3, k_feat 8192
X_train.shape:  (265, 8192)
y_pred:  [ 0.82222329  0.73493899  0.75990874  0.47257893  0.06700034  0.84211741
  0.54762351 -0.45425248 -0.82325838  0.02045321  0.72102372  0.76848454
 -0.47635684 -0.16485413  0.11689775 -0.16755044  0.1165758  -0.61958898
  0.13229528  0.50302416 -0.69336285 -0.43437612 -0.27032705 -0.06867222
 -0.06008478 -0.68503481 -1.17242596 -0.60733705 -0.59120947 -0.27258323
  0.65625856  0.5392279   0.707226   -0.59332482 -0.13145802 -0.57595382
  0.36024932 -0.46872313  0.07306901  0.60578811 -0.16650517  0.02349733
  0.02493423 -0.00775167  0.76710807 -0.46082429  0.76551226  0.75306083
  0.01148664  0.1406417  -0.08219847  0.41759852  0.26229304  0.2609682
  0.51266311  0.16836     0.08484663  0.36811989  0.36901451 -0.28820658
  0.8036736  -0.50291205  0.89283568  0.54719835 -0.24344643 -0.19142243
 -0.09096081 -0.04631115 -0.17047268 -0.05602901  0.7621784   0.49717895
 -0.2925032  -0.2497691  -0.64785061  0.11751066 -0.04176279 -0.07208854
  0.11176366  0.09352113  0.11109535  0.11828606  0.11577068 -0.25514126
  0.03426488 -0.0665396   0.27997753  0.03912705  0.04735406 -0.03231042
 -0.17064956  0.11537168 -0.19519729  0.30562739 -0.59849921 -0.65292367
 -0.59812622 -0.67507805  0.00293556 -0.03352994  0.54272377  0.64268474
  0.76016228  0.61917937  0.31572662 -0.46790033  0.53910523 -0.59410106
 -0.17601702 -0.5263732  -0.60929752 -0.85259411 -0.48796253 -0.78069317
 -0.81920628 -0.80980748 -0.59667797 -0.20649473 -0.68270195 -0.74016937
 -0.68618584 -0.58134838 -0.08803114 -0.48889409 -0.99003552 -0.61412295
 -0.53802661 -0.35610744 -0.48795214 -0.73712976 -0.54891192 -0.64632756]
Pearson r: 0.703
MSE: 0.424
Average pearson_r accross all folds = 0.7030524946580106
Average mse accross all folds = 0.5811932849136332
Fold 1, k_feat 16384
X_train.shape:  (264, 16384)
y_pred:  [ 0.42656498  0.38677001  0.42702353  0.5225176   0.59788424  0.39235438
  0.34724834  0.33758379  0.33863634  0.34399637  0.20400911  0.24881165
 -0.64306407 -0.61347488 -0.95836083  0.0071123   0.26648192  0.3445812
 -0.35788373 -0.33284769  0.07091049  0.34780193 -0.45967972 -0.27673291
 -0.66428586 -0.65304575  0.30668054 -0.45009525 -0.6601255   0.23904062
  0.18552863 -0.55412573  0.283768    0.19768871  0.40634441 -0.10382614
  0.13724616 -0.14292403 -0.95502651 -0.13089438 -0.3251125  -0.48424923
 -0.61039681 -0.4865219  -0.51757261  0.02213953  0.36809566 -0.5206255
 -0.51156911 -0.60778171 -0.06935675 -0.22989845 -0.3795687  -0.36131048
 -0.06916863 -0.0385684  -0.55609407 -0.93157923 -0.62145565  0.17774309
 -0.21988289 -0.95897007 -0.1945375   0.16512283  0.08374792  0.00773549
  0.40717048  0.19417188  0.16690176 -0.25327428 -0.26184473 -0.30340245
  0.53779961  0.18397689  0.23903535 -0.47964821 -0.66877217 -0.24592304
  0.06586286  0.01216007 -0.069238    0.04627468  0.07211656 -0.03512822
 -0.10316639 -0.38508871  0.06309812  0.03610043 -0.11250689  0.06004628
 -0.38116019  0.06511989  0.04891901 -0.29058194  0.40035539  0.47300144
 -0.66343212 -0.66401924  0.33811749  0.47833088  0.56319693 -0.51012098
  0.35949842 -0.12201233 -0.60795117 -0.57025202 -0.44138941 -0.46004362
 -0.52078982 -0.13244008 -0.51650999 -0.38093329 -0.49325844 -0.56534416
 -0.30652029 -0.57785395 -0.63122398 -0.39027116 -0.61374991 -0.33329218
 -0.66938738 -0.42774133 -0.80158585 -0.56474441 -0.50804834 -0.68326808
 -0.53575709 -0.57515605 -0.48450613 -0.51146779 -0.59497346 -0.55609899
 -0.55967078]
Pearson r: 0.733
MSE: 0.663
Fold 2, k_feat 16384
X_train.shape:  (265, 16384)
y_pred:  [ 0.23750383  0.09064028  0.25941055  0.45955284  0.13728901  0.17963981
 -0.41175496  0.28960586  0.15206634  0.00764729 -0.08919062 -0.13467009
 -1.17275179  0.39207618 -0.40346819 -0.50886631 -0.55846503 -0.17125851
 -0.51299293 -0.22832222 -0.28129631 -0.09117783  0.38415973  0.0512502
  0.3928377  -0.65616228 -0.11481946 -0.00262743 -0.06431428 -0.25100257
 -0.37395469 -0.47564581  0.30542712 -0.39683013 -0.7622259  -0.30085291
 -0.42831901 -0.43057548  0.54623176  0.50062346  0.01576534  0.37459702
 -0.20022674 -0.13847632 -0.1853571   0.02185602 -1.1846208  -1.1834347
 -0.59564433 -0.06767005 -0.43218003 -1.10436508 -0.51913613 -0.59536357
  0.26150948  0.23869196  0.43027043 -0.21741106  0.45084363 -0.2343314
  0.12253083  0.12670394  0.24731932  0.1007315   0.09967222  0.12117978
  0.04061728 -0.18574569  0.01557603  0.40340623  0.36610722 -0.06498774
  0.05468575  0.41014309 -0.13017141 -0.17550031  0.38132933  0.48821298
  0.42795699  0.10417105 -0.62017833  0.14281304 -0.28937669 -0.21751155
 -0.57473158 -0.07594217 -0.3793096  -0.11575941 -0.37684802  0.12912803
  0.12506412 -0.19430045 -0.06603366 -0.22899015 -0.18875053  0.112496
  0.35169557  0.3966985   0.40956929 -0.16679455  0.39898707  0.32928133
 -0.50810462 -0.38322845 -0.51303755 -0.50639665 -0.41603624 -0.36351479
  0.44966655 -0.40134144  0.31645167 -0.69048223 -0.59934874 -0.23740152
 -0.77614418 -0.87906899 -0.83767132 -0.37249453 -0.6510565  -0.60547502
 -0.644275   -0.71302234 -0.78361763 -0.63008149 -0.75685684 -0.69351808
 -0.60814113 -0.6628557  -0.21844121 -0.58205038 -0.5241308  -0.43635948]
Pearson r: 0.669
MSE: 0.761
Fold 3, k_feat 16384
X_train.shape:  (265, 16384)
y_pred:  [ 0.61986652  0.58424139  0.55028852  0.29403573 -0.08137887  0.66393928
  0.39897981 -0.45861384 -0.75707934  0.04527773  0.5281548   0.58911333
 -0.39337524 -0.18678967  0.09546229 -0.18860161  0.09582788 -0.52778362
  0.01217077  0.37089909 -0.63485862 -0.36614507 -0.29932708 -0.07356964
 -0.04181034 -0.57427338 -1.16594213 -0.59201493 -0.57552536 -0.22892014
  0.50660075  0.4034692   0.56099589 -0.59232756 -0.15223962 -0.51152325
  0.23471508 -0.40525462  0.04134694  0.47157421 -0.17839034 -0.00789042
 -0.00693648 -0.01985959  0.5684318  -0.4043105   0.6149138   0.59930219
 -0.02001167  0.05071884 -0.07066478  0.27233904  0.17034457  0.15643771
  0.36329022  0.13982826  0.03557655  0.27596939  0.27621898 -0.28045312
  0.62270788 -0.48914751  0.70497588  0.45195058 -0.18920162 -0.20164419
 -0.12074319 -0.0862889  -0.17976781 -0.05207256  0.58863009  0.38404065
 -0.33170733 -0.27431521 -0.55101218  0.11381776 -0.04945271 -0.12803921
  0.09796554  0.08649787  0.09803407  0.10812307  0.11159058 -0.26042359
  0.01085124 -0.11196496  0.24067246  0.01185508  0.04120404 -0.04119775
 -0.21018304  0.10008302 -0.1900003   0.17373713 -0.50595666 -0.54866349
 -0.51634344 -0.56418666 -0.01775736 -0.04545874  0.39395332  0.47144366
  0.54861381  0.44007006  0.15960533 -0.40859927  0.42357085 -0.59298693
 -0.18655809 -0.504875   -0.61210921 -0.83332491 -0.50248676 -0.75159414
 -0.74282362 -0.73298323 -0.56928488 -0.27034507 -0.70615788 -0.70657359
 -0.68269348 -0.58328378 -0.13938257 -0.48258709 -0.97710587 -0.56624058
 -0.50227471 -0.36076915 -0.43576413 -0.69264187 -0.55389791 -0.65807361]
Pearson r: 0.695
MSE: 0.462
Average pearson_r accross all folds = 0.6988910151086758
Average mse accross all folds = 0.6285614527870806
