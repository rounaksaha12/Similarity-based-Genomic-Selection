After clean(removal of rows with high missing values + redundancy removal) X.shape:  (397, 24108)
Fold 1, k_feat 8
X_train.shape:  (264, 8)
y_pred:  [ 0.9688947   0.59091323  0.9688947   0.9688947   0.9688947   0.9688947
  0.9688947   0.9688947   0.9688947   0.9688947  -0.24271575  0.9688947
  0.09033252 -1.01869469 -0.92544291 -0.24271575  0.9688947   0.72128274
  0.70858958 -0.69495508  0.32730521  0.9688947  -0.50167199 -0.38034391
 -0.92544291  0.09033252  0.9688947  -0.52815517 -0.92544291  0.9688947
  0.9688947   0.09033252  0.9688947   0.9688947   0.9688947   0.09033252
  0.32730521 -0.361695   -0.92544291 -0.22182028 -0.12080758 -0.40634143
 -0.7273459  -0.92544291 -0.92544291 -0.5097389   0.9688947  -0.92562822
 -1.01869469 -0.92544291  0.71482802 -1.01869469 -0.50167199 -0.55784646
  0.09033252  0.09033252  0.09033252 -0.92544291 -1.01869469  0.9688947
 -0.69495508 -0.92544291 -0.92562822  0.59091323  0.32730521  0.01433487
  0.32730521 -0.66971946  0.32730521 -0.77670888 -0.49679641 -0.51006439
  0.9688947   0.9688947   0.9688947  -0.37428932 -0.92544291 -0.89387174
  0.09033252  0.09033252  0.09033252  0.09033252  0.09033252  0.09033252
  0.09033252  0.46891917  0.09033252  0.09033252 -0.74831292  0.09033252
 -0.51006439  0.09033252  0.09033252 -0.69495508  0.9688947   0.9688947
 -0.92544291 -0.92544291  0.9688947   0.9688947   0.9688947  -0.92544291
  0.9688947  -0.89387174 -0.37428932  0.61492342 -0.49679641 -0.71817658
  0.70858958  0.59091323 -0.50167199 -0.66971946 -0.7273459  -0.74831292
 -0.66971946 -0.49679641  0.23204167  0.02364065 -1.11727814 -0.93211472
 -0.33796452 -0.35203801 -0.92544291 -0.82485074 -0.98728049 -0.92562822
 -0.92562822 -0.49481357  0.89502547 -0.56812135 -0.49679641 -0.86267731
 -0.54710913]
Pearson r: 0.612
MSE: 0.696
Fold 2, k_feat 8
X_train.shape:  (265, 8)
y_pred:  [ 1.05348491  1.32407542  1.05348491  1.05348491  0.75927541  1.05348491
  0.28412994 -0.22574775 -0.01587185  1.05348491  1.05348491  1.05348491
 -0.92837901  1.05348491 -0.92837901 -0.12984243 -0.6508603   0.31289001
 -0.92837901  0.01309582 -0.87100883 -0.01587185  1.05348491  1.05348491
  1.05348491 -0.69551447 -0.00633692  1.05348491 -0.22574775 -0.75091057
 -0.7603833  -0.92837901  1.05348491  0.19938213 -0.34170414  0.01509323
 -0.34170414 -0.92837901  1.05348491  1.05348491  1.05348491  1.05348491
 -0.17243476  0.31289001  0.09974978  0.05898402 -0.92837901 -0.92837901
 -0.74207298 -0.24684446 -0.92837901 -0.92837901 -0.92837901 -0.74207298
  1.05348491  1.05348491  1.05348491  0.75927541  1.05348491  0.75927541
 -0.01587185  1.05348491  1.05348491  1.05348491  0.28412994  1.05348491
  1.05348491  0.3630571   1.05348491  1.05348491  1.05348491  0.21419796
 -0.30104869  1.05348491 -0.22574775 -0.06965824  1.05348491  1.05348491
  1.05348491  0.4321769  -0.8983508   1.05348491 -0.69854338  0.31289001
 -0.44538773 -0.24684446 -0.92837901  0.31289001 -0.35808112  0.28412994
  0.13416661  0.20261554 -0.27176766 -0.25010005 -0.09995978  0.28412994
  1.05348491  1.05348491  1.05348491 -0.20112534  1.05348491 -0.01587185
 -0.92837901 -0.37769972 -0.92837901 -0.92837901 -1.10296477 -0.80239973
  1.05348491 -0.3678082   1.05348491 -0.92781062 -0.35633858 -0.27064015
 -0.85960422 -0.27176766 -0.22927914  0.47348884 -0.73032018 -0.52575987
 -0.87747736 -0.27176766 -0.92781062 -0.5259793  -0.6512418  -0.03376652
 -0.51241884 -0.69490658  1.05348491 -0.30146037 -0.61286001 -0.17503337]
Pearson r: 0.641
MSE: 0.673
Fold 3, k_feat 8
X_train.shape:  (265, 8)
y_pred:  [ 8.66459773e-01  8.66459773e-01  8.66459773e-01  8.66459773e-01
 -2.34254689e-01  8.66459773e-01  8.66459773e-01 -7.61101192e-01
 -1.10392028e+00  7.45316883e-01  8.66459773e-01  8.66459773e-01
 -4.85621221e-01 -7.09670452e-02  2.94679972e-01 -7.09670452e-02
  2.94679972e-01  8.44609452e-02  8.66459773e-01  8.66459773e-01
 -3.12179984e-01 -6.82283936e-01  3.62960233e-01 -8.67041543e-01
 -3.12179984e-01 -1.13207310e+00 -6.64860165e-01 -2.41840045e-01
 -2.41840045e-01  8.66459773e-01  8.66459773e-01  8.66459773e-01
  8.66459773e-01 -2.41840045e-01 -7.63492474e-01 -6.64860165e-01
  8.66459773e-01 -1.06155132e+00 -8.54295163e-01  8.66459773e-01
 -1.03921321e+00  2.94679972e-01  2.94679972e-01 -4.92686491e-01
  8.66459773e-01 -6.82283936e-01  8.66459773e-01  8.66459773e-01
  2.94679972e-01  8.66459773e-01  4.73427446e-01  8.66459773e-01
  8.66459773e-01  8.66459773e-01  8.66459773e-01  2.06087552e-01
 -6.14846397e-01  8.66459773e-01  8.66459773e-01 -7.22996588e-01
  6.82626590e-01 -2.41840045e-01  8.66459773e-01 -3.89574264e-01
 -6.33367484e-02  2.94679972e-01 -1.65983697e-01 -4.29120638e-01
 -2.74303377e-01 -1.99002491e-01  8.66459773e-01  8.66459773e-01
  1.65979164e+00 -2.85807801e-01 -6.82283936e-01  2.94679972e-01
 -3.12179984e-01 -3.79178361e-01  2.94679972e-01 -7.09670452e-02
  2.94679972e-01  2.94679972e-01 -7.09670452e-02 -4.54027476e-01
  2.94679972e-01 -1.82234183e-01 -1.25126605e+00 -1.82234183e-01
  2.94679972e-01 -1.99002491e-01 -5.78052513e-01  2.94679972e-01
  3.62960233e-01  8.66459773e-01 -1.13207310e+00 -1.13207310e+00
 -6.82283936e-01 -1.13207310e+00  3.62960233e-01 -1.82234183e-01
  8.66459773e-01  8.66459773e-01  8.66459773e-01  1.39076298e-03
  8.66459773e-01 -1.13207310e+00  8.66459773e-01 -2.41840045e-01
  2.50852430e-01  4.73427446e-01 -5.34887820e-01 -9.48418411e-01
 -7.59819391e-01 -1.25126605e+00 -2.74303377e-01 -3.12179984e-01
 -1.25126605e+00 -6.95970031e-01 -9.62513894e-01 -6.82090129e-01
 -1.10392028e+00 -4.48262349e-01  6.38704600e-01 -8.67041543e-01
 -4.99678775e-01 -5.32648667e-01 -4.41954985e-01  2.88516868e-02
 -5.48129181e-01 -4.99678775e-01 -3.24096830e-01 -6.14846397e-01]
Pearson r: 0.586
MSE: 0.550
Average pearson_r accross all folds = 0.6129948780002225
Average mse accross all folds = 0.6398019469078323
Fold 1, k_feat 16
X_train.shape:  (264, 16)
y_pred:  [ 1.02765879  0.7195178   1.02765879  1.02765879  1.02765879  1.02765879
  1.02765879  1.02765879  1.02765879  1.02765879 -0.21108513  1.02765879
 -0.54414248 -1.10607015 -1.24102377  0.18220313  0.7712332   0.59153656
  0.7582156  -0.42967334  0.36703343  1.02765879 -0.70502327 -0.70386172
 -1.00929429 -0.54414248  1.02765879 -0.61381211 -1.00929429  0.27442428
  0.27442428 -0.54414248  1.02765879  0.27442428  1.02765879 -0.17915754
  0.21393209 -0.64981557 -1.24102377 -0.024057   -0.92132924 -0.32883753
 -0.71911867 -0.94803908 -0.94803908  0.07777033  1.02765879 -0.63209397
 -0.74453172 -0.57773401 -0.39794618 -1.05585301 -0.65934761 -0.77989513
  0.27061879  0.27061879 -0.54414248 -1.36129473 -1.10607015  1.02765879
  0.21069516 -1.24102377 -0.51964658  0.7195178   0.21393209 -0.10604033
  0.21393209  0.00171099  0.21393209 -0.52940651 -0.20444155 -0.67715709
  1.02765879  0.988449    0.27442428 -0.32483184 -1.00929429 -0.68514606
  0.27061879  0.27061879  0.27061879  0.27061879  0.27061879 -0.16246964
  0.27061879 -0.02532241  0.27061879  0.27061879 -0.63340465  0.27061879
  0.06762735  0.27061879  0.27061879 -0.62662312  1.02765879  1.02765879
 -1.00929429 -1.00929429  1.02765879  1.02765879  1.02765879 -0.57773401
  1.02765879 -0.15674334 -0.96440419 -0.68781389  0.36627513 -0.58551449
 -0.04143869  0.7195178  -0.24301694 -0.700713   -0.84040934 -0.64915284
 -0.67390452 -0.18423036  0.12801647 -0.02955184 -0.37520467 -0.43913028
 -0.36121751 -0.91986858 -0.4255164  -0.83542842 -1.05181245 -0.51964658
 -0.65688833 -0.61803951  0.0044783  -0.44099513 -0.20668727 -0.38603821
 -0.46748442]
Pearson r: 0.704
MSE: 0.558
Fold 2, k_feat 16
X_train.shape:  (265, 16)
y_pred:  [ 1.07058175  1.18246772  0.35630868  1.07058175  0.31726292  1.07058175
  0.09834181 -0.12098613  0.33072889  0.35630868  1.07058175  1.07058175
 -0.9148021   1.07058175 -0.9148021  -0.20283388 -0.52834381  0.40993737
 -0.9148021   0.08041333 -0.82296323 -0.26325511  1.07058175  1.07058175
  1.07058175 -0.67927796 -0.22633905  1.07058175  0.3279637  -0.72977978
 -0.4948523  -0.9148021   0.83526921 -0.01794343 -0.38495777 -0.01429491
  0.0066142  -0.9148021   1.07058175  1.07058175  1.07058175  0.83526921
  0.2178875   0.40993737  0.09691789  0.53513464 -0.9148021  -0.9148021
 -0.33411843 -0.26292506 -0.9148021  -0.9148021  -0.9148021  -0.33411843
  1.07058175  0.35630868  1.07058175  0.23282389  1.07058175  0.23282389
 -0.26325511  0.35630868  1.07058175  0.35630868  0.27372251  1.07058175
  0.98452822  0.29270313  1.07058175  1.07058175  1.07058175 -0.05464662
 -0.26786892  1.07058175  0.3279637   0.24711202  1.07058175  1.07058175
  1.07058175  0.70768621 -0.57933908  1.07058175 -0.4747761  -0.1782296
 -0.14758847 -0.26292506 -0.9148021   0.24954355 -0.40681662  0.27372251
  0.35027669  0.31133712  0.16969259 -0.09382873 -0.17017388  0.27372251
  1.07058175  0.83526921  0.83526921 -0.07974848  1.07058175  0.33072889
 -0.9148021  -0.42583265 -0.9148021  -0.9148021  -0.71356929 -0.62503495
  1.07058175 -0.43743784  1.07058175 -0.60696505 -0.25440545 -0.14941106
 -0.73062151 -0.87930499 -0.2486678   0.03308231 -0.4800469  -0.95454379
 -0.77513805  0.09333021 -0.71918087 -0.49778061 -0.49722116 -0.69548839
 -0.59386088 -0.54741168  0.48943815 -0.10984713 -0.65995005 -0.17114034]
Pearson r: 0.687
MSE: 0.599
Fold 3, k_feat 16
X_train.shape:  (265, 16)
y_pred:  [ 1.13861635  1.13861635  1.13861635  1.13861635 -0.54169613  1.13861635
  0.3686064  -0.62083992 -1.06607033  0.51553923  1.13861635  1.13861635
 -0.6792669  -0.32031645  0.18382898 -0.19831197  0.18382898 -0.87434315
  1.13861635  0.05297808 -0.54099529 -0.77653934 -0.60548541 -0.46481017
 -0.60290071 -1.0984296  -0.45720655 -0.46071113 -0.46071113  0.3686064
  0.3686064   0.3686064   1.13861635 -0.46071113 -0.73568643 -0.78110856
  0.3686064  -0.97989735 -1.04927806  1.13861635 -1.02109153  0.77040429
  0.77040429 -0.23789505  1.13861635 -0.40130023  1.13861635  0.05297808
  0.55315328  1.13861635  0.06611941  0.05297808  1.13861635  1.13861635
  1.13861635  0.35400825 -0.55161897  1.13861635  1.13861635 -0.69308174
  0.00937625 -0.46071113  1.13861635 -0.57878289  0.22567658  0.18570191
 -0.38583649  0.12510596 -0.56794074 -0.27360383  1.13861635  1.13861635
  1.63207898 -0.16043295 -0.77653934  0.18382898 -0.30270354 -0.52579426
  0.18382898  0.16129319  0.18382898  0.18382898  0.16129319 -0.51447662
  0.19897779 -0.07154063 -0.61878115 -0.07154063 -0.02889167  0.54506848
 -0.22613023  0.18382898 -0.51106386  0.05297808 -1.0984296  -1.0984296
 -0.40130023 -1.0984296  -0.04955283 -0.11074608  1.13861635  1.13861635
  1.13861635 -0.41716117  1.13861635 -1.0984296   0.38950255 -0.46071113
  0.11637926  0.86821343 -0.39435072 -0.96384235 -0.50695897 -1.09036823
 -0.6153313  -0.58040741 -1.38232319 -0.69698497 -0.85933144 -0.67538078
 -0.58997773 -0.29310401 -0.56323065 -0.68119912 -0.73051068 -0.53698931
 -0.13126731 -0.04057728 -0.46351716 -0.73231331 -0.3153959  -0.57766957]
Pearson r: 0.621
MSE: 0.519
Average pearson_r accross all folds = 0.6705378690974891
Average mse accross all folds = 0.5587733645303526
Fold 1, k_feat 32
X_train.shape:  (264, 32)
y_pred:  [ 0.19418262  0.97651693  0.95897808  0.99761108  0.95897808  0.95144816
  0.95144816  0.92105304  0.95144816  0.95897808 -0.17339121  0.72625895
 -0.41867033 -1.15194903 -1.268143   -0.2627386   0.93975878  0.76520109
  0.8836246  -0.40699459  0.39169715  0.95144816 -0.73613493 -0.66230033
 -1.00929429 -0.41867033  0.95897808 -0.64738092 -1.00929429  0.06180565
  0.05340184 -0.41867033  1.66757625  0.55295196  0.92858295 -0.07679096
  0.00287361 -0.68809861 -1.268143    0.105803   -0.60868388 -0.40835915
 -0.57309456 -0.89592987 -0.96416229  0.58175419  0.95897808 -0.4551378
 -0.74771747 -0.65799976  0.10239212 -0.79244085 -0.6259342  -0.5873527
  0.33045604  0.33045604 -0.41867033 -1.70937724 -1.15194903  0.72625895
  0.25754645 -1.268143   -0.20057456  0.97651693  0.00287361 -0.20444807
  0.00287361  0.34782476  0.00287361 -0.16504924  0.63943774 -0.49725424
  0.95897808  1.63723623  0.06180565 -0.44324915 -1.00929429 -0.78414292
  0.33045604  0.25533348  0.08635356  0.33045604  0.25533348  0.02842296
 -0.02297412 -0.30660686  0.33045604  0.33045604 -0.7651829   0.33045604
 -0.32337352  0.33045604  0.33045604 -0.63712412  0.99761108  0.19418262
 -1.00929429 -1.00929429  1.66762806 -0.05569242  1.77522531 -0.65799976
  0.95897808 -0.31169336 -0.67328297 -0.57550351  0.7234786  -0.77527992
  0.04962697  0.94509087 -0.42949475 -0.7328054  -0.83064426 -0.71790496
 -0.59581571  0.29782876 -0.65828047 -0.24590262 -0.75722053 -0.58047203
 -0.37840346 -0.66768305 -0.76528644 -0.84432925 -1.03575333 -0.71001613
 -0.86949198 -0.60200119  0.82944792 -0.57713532  0.31971582 -0.43502756
 -0.33764708]
Pearson r: 0.623
MSE: 0.682
Fold 2, k_feat 32
X_train.shape:  (265, 32)
y_pred:  [ 1.15139936e+00  1.52578108e+00  5.77732913e-01  1.15139936e+00
  3.89767331e-02  1.15139936e+00  1.11721611e-03  2.92976692e-01
  7.43799379e-01  6.01152856e-01  1.25612597e+00  1.25612597e+00
 -9.85643185e-01  1.15139936e+00 -1.08453937e+00 -4.78750519e-01
 -6.91617766e-01  1.63981243e-01 -1.04387767e+00  6.14703207e-02
 -7.51040658e-01  1.38578832e-02  1.15139936e+00  1.15139936e+00
  1.15139936e+00 -5.02520961e-01 -2.39277250e-01  6.75136651e-01
  2.18910251e-01 -4.63459706e-01 -6.00516580e-01 -1.00359504e+00
  8.58457828e-01 -1.42991401e-02 -1.41930323e-01  1.84104817e-01
 -2.40680732e-01 -1.00359504e+00  1.15139936e+00  1.15139936e+00
  7.36107830e-01  8.58457828e-01  2.85824408e-02  1.63981243e-01
  2.31704466e-01  6.13475057e-01 -1.00359504e+00 -9.85643185e-01
 -4.61994620e-01 -8.78356985e-01 -4.17970011e-01 -1.00359504e+00
 -1.00359504e+00 -4.61994620e-01  1.15139936e+00  1.13285697e-01
  1.15139936e+00 -1.37705941e-01  1.15139936e+00 -1.37705941e-01
 -4.03605184e-01  5.77732913e-01  1.15139936e+00  7.04780690e-01
  2.04640334e-01  1.25612597e+00  4.67164573e-01  3.97578993e-01
  7.36107830e-01  1.15139936e+00  1.15139936e+00 -3.61565112e-02
 -3.93941271e-01  1.15139936e+00  7.47533130e-02 -1.83116268e-01
  1.15139936e+00  1.15139936e+00  1.15139936e+00  4.99107531e-01
 -5.89003206e-01  2.62667307e-01 -4.79452061e-01 -3.80268870e-02
 -3.35380890e-02 -8.78356985e-01 -4.17970011e-01  1.93332100e-02
 -2.88900597e-01  2.94594756e-01  3.05676631e-01  8.06229978e-02
  4.74888561e-02 -3.46962403e-01 -7.94884594e-02  1.47540083e-01
  1.15139936e+00  8.58457828e-01  8.58457828e-01 -3.34310670e-01
  1.15139936e+00  7.43799379e-01 -1.00359504e+00 -5.28785070e-01
 -1.00359504e+00 -1.00359504e+00 -8.96561205e-01 -4.73055438e-01
  1.15139936e+00 -8.38900087e-01  1.15139936e+00 -7.18531276e-01
 -3.70571022e-01 -4.68696933e-01 -6.25538911e-01 -5.17956803e-01
 -4.83622233e-01 -2.15022656e-01 -5.47255580e-01 -9.85719039e-01
 -6.21293998e-01  3.59130828e-01 -7.72618286e-01 -1.16533739e-01
  1.07867735e-01 -9.49557163e-01 -8.34524747e-01 -5.95731481e-01
  8.04514766e-01  1.17531571e-01 -8.88407795e-02  9.92013669e-02]
Pearson r: 0.675
MSE: 0.615
Fold 3, k_feat 32
X_train.shape:  (265, 32)
y_pred:  [ 1.23475253  1.23475253  1.0671445   1.0671445  -0.17653812  0.23864858
  0.23522689 -0.41999319 -1.100886    0.16540552  0.23864858  1.44921661
 -1.03487811 -0.16716985  0.06874151 -0.13091363  0.06874151 -0.92500554
  1.61642314  0.74951128 -0.44496249 -0.67442502 -0.54878348 -0.48672444
 -0.18271094 -0.80229437 -0.53773204 -0.4951063  -0.83835218  0.09758539
  0.23522689  0.15649174  1.23475253 -0.50895279 -0.68860339 -1.03345059
  0.15649174 -1.20577365 -0.85099073  1.23475253 -0.81214014  0.80139689
  0.80139689 -0.2488683   1.23475253 -0.48645924  1.23475253  0.74951128
  0.56619308  1.61642314 -0.22878186  0.1226521   0.94964312  1.1173524
  1.23475253  0.23584695 -0.54737414  1.0671445   1.0671445  -0.60725303
  0.15283044 -0.55903584  1.23475253 -0.64138961  0.32998681  0.57136596
 -0.32947827  0.32598038 -0.55602639 -0.2515942   1.23475253  1.23475253
  0.84378224 -0.21932247 -0.92270019  0.22167647 -0.52427209 -0.4540085
  0.22167647  0.13127666  0.22167647  0.27568874  0.17728488 -0.43471183
  0.46727205  0.01367926 -0.2924608   0.01367926  0.0525671   0.33205146
 -0.27896301  0.18089298 -0.46536053  0.6943263  -1.15905379 -1.29366039
 -0.49668601 -1.29366039  0.07667262 -0.22409231  1.23475253  1.23475253
  1.23475253 -0.44708795  1.61642314 -1.03237716  0.27108547 -0.50895279
  0.00336821  0.22422468 -0.7884605  -1.04899084 -0.25330841 -0.58319341
 -0.30066586 -0.75623816 -0.81151341 -0.73255234 -0.40829148 -0.71211323
 -0.42376225 -0.48543305  0.05634479 -0.49687366 -0.8666539  -0.52721822
 -0.48711908  0.33634708 -0.51578742 -0.71847666 -0.37092925 -0.66767182]
Pearson r: 0.604
MSE: 0.544
Average pearson_r accross all folds = 0.6340320441173763
Average mse accross all folds = 0.6135688014106825
Fold 1, k_feat 64
X_train.shape:  (264, 64)
y_pred:  [ 0.45112618  0.75290085  0.98454957  0.9234169   1.02101945  1.02101945
  1.02101945  0.68298683  1.02101945  1.02101945  0.03624443  0.16248381
 -0.41245389 -1.14780469 -1.2630317   0.43339914  0.87975674  0.7434539
  0.80092003  0.0084275   0.31142215  1.02101945 -0.86855704 -0.60914997
 -0.98762508 -0.41245389  1.02101945 -0.52051378 -0.96655706  0.09259776
  0.01559035 -0.46576248  1.94909666  0.53246461  0.68298683  0.25731831
 -0.11922312 -0.72353235 -1.17372219 -0.01105059 -0.8623145  -0.42949498
 -0.96709263 -1.04264314 -1.2196563   0.33673315  1.02101945 -0.24144759
 -0.77211511 -0.95201335  0.37263341 -0.57775695 -0.51963319 -0.58583835
  0.60124305  0.68138298 -0.46576248 -1.5335659  -1.02803463  0.2168736
  0.27949044 -1.17372219 -0.56905404  0.11538697 -0.11922312 -0.04521504
  0.1701953   0.1999259  -0.11922312  0.00814236  0.39237042 -0.49849739
  1.02101945  1.34610061  0.09259776 -0.56926156 -1.22765055 -0.69866676
  0.49897741 -0.08464818 -0.10663337  0.15226048  0.17729649 -0.04207133
  0.07965758 -0.44596841  0.41734509  0.29145134 -0.5833518   0.29145134
 -0.53074533 -0.05513585  0.29145134 -0.76300691  0.9234169   0.45112618
 -0.96655706 -0.96655706  1.94194776  0.04375768  1.96737664 -0.6599174
  1.02101945  0.40091819 -0.86568175 -0.27511983  0.29763176 -0.7577229
 -0.01909062  0.84188496 -0.3382019  -0.56307062 -0.52906152 -0.07728172
 -0.61609168  0.06124134 -0.70146398 -0.15731835 -0.76518274 -0.62251533
 -0.53983659 -0.69058547 -1.14873724 -0.35834771 -1.01104293 -0.60552369
 -0.86679948 -0.60977166  0.36188395 -0.56843225  0.14542227 -0.26724574
 -0.26895538]
Pearson r: 0.645
MSE: 0.648
Fold 2, k_feat 64
X_train.shape:  (265, 64)
y_pred:  [ 1.36549741  1.26930218  1.00090294  1.36549741  0.07994303  1.3240496
  0.04380494  0.11641137  0.99102949  1.33541305  1.26692029  1.23200529
 -1.50186859  1.36549741 -1.08463175 -0.28710781 -0.5880791   0.17590904
 -0.93437948 -0.1182743  -0.70398334  0.40558024  1.36549741  0.80488875
  0.80488875 -0.59239579 -0.09507545  0.56378592  0.27584887 -0.34666601
 -0.34504271 -0.81170719  0.78797333 -0.06354299 -0.43721115  0.03167293
 -0.30630605 -0.81170719  1.36549741  1.36549741  0.72340628  0.94996012
  0.12049325  0.17590904 -0.07792981  0.64590127 -1.75268655 -1.50186859
 -0.62939943 -0.75618609 -0.26653299 -1.75268655 -0.81170719 -0.62939943
  1.36549741  0.33371696  0.29455827 -0.30900765  1.36549741 -0.23450165
 -0.40313757  0.43452224  1.36549741  0.92480966  0.0403282   1.07194892
  0.90793425  0.50599202  0.72340628  0.06784358  1.36549741 -0.01862107
 -0.20044393  1.36549741  0.25455691 -0.06117373  0.11270936  1.36549741
  1.03117437 -0.08981815 -0.58128767  0.2814945  -0.4136746  -0.10055322
 -0.14643512 -0.75618609 -0.30436276 -0.11149406 -0.48786911  0.32913532
  0.15672177  0.15586025  0.40599072 -0.23972348 -0.39419087  0.24799714
  0.80488875  0.32319886  0.32319886 -0.31134591  1.36549741  0.42752191
 -0.81170719 -0.50215717 -0.81170719 -0.81170719 -0.63563414 -0.39122953
  1.36549741 -0.81607     1.36549741 -0.73344293 -0.45794188 -0.16833471
 -0.60235581 -0.80095704 -1.00972316 -0.24410321 -0.57632547 -1.01517861
 -0.49441556 -0.02827984 -0.85356338 -0.38218891  0.22383532 -0.91688672
 -0.86429604 -0.6942415   0.35910178  0.05606673  0.20326051 -0.10442771]
Pearson r: 0.656
MSE: 0.647
Fold 3, k_feat 64
X_train.shape:  (265, 64)
y_pred:  [ 1.22493596  0.80569234  1.19050601  0.83570996 -0.14737212  0.24227191
  0.17945298 -0.32342895 -1.15774507  0.03565427  0.17884094  1.54994284
 -0.97137164 -0.27013718  0.09175021 -0.28751896  0.09175021 -1.02723892
  1.95732493  0.80096708 -0.45062053 -0.72045052 -0.4641725  -0.4397495
 -0.16838748 -0.73549859 -0.69167609 -0.62653956 -0.70846021  0.73081292
  0.23522689 -0.02735977  1.22493596 -0.47271617 -0.30847273 -0.773168
 -0.02735977 -0.97213299 -0.90696604  1.14845264 -0.73579963  0.9222459
  0.83592617 -0.14953266  1.22493596 -0.63226001  1.22493596  0.95010539
  0.73752771  1.95732493  0.00719797  0.48348186  1.0960956   0.61277481
  1.22493596  0.01521121 -0.47173389  1.19050601  1.19050601 -0.44060391
  0.5234545  -0.54680962  1.22493596 -0.12860693  0.18288911  0.55851481
 -0.22333154  0.01053687 -0.42125796 -0.27650811  1.22493596  1.22493596
  0.18104038 -0.29344801 -0.77681859  0.31653474 -0.14224121 -0.36326967
  0.23640175  0.14448467  0.23640175  0.11826453  0.20900339 -0.44089861
  0.34570074 -0.02609266  0.31751691 -0.02933333  0.24818592  0.0212286
 -0.16308448  0.16275017 -0.39493988  0.48686654 -0.85468551 -0.8145209
 -0.60363409 -0.99216625  0.06248586 -0.12360731  1.22493596  1.22493596
  1.22493596 -0.18357943  1.95732493 -0.99779727  0.21808245 -0.47271617
  0.05286206 -0.05167431 -0.6739617  -0.90048459 -0.50034019 -0.61772125
 -0.45545203 -0.70074053 -0.54283234 -0.40669461 -0.66083041 -0.76499022
 -0.4328263  -0.44796897  0.42105641 -0.51580216 -0.83807941 -0.25875807
 -0.577596    0.50213688 -0.51057291 -0.58424413 -0.35119069 -0.60764574]
Pearson r: 0.603
MSE: 0.539
Average pearson_r accross all folds = 0.6346924666243184
Average mse accross all folds = 0.6113294203873304
Fold 1, k_feat 128
X_train.shape:  (264, 128)
y_pred:  [ 0.76400449  1.61452283  1.64789765  0.92498489  1.84253452  0.6997619
  0.6997619   0.69970269  0.6997619   0.88012206  0.1315235   0.99410621
 -0.78977393 -1.09651907 -1.26503478  0.57585921  0.85239743  0.98785343
  0.58726623  0.03021093  0.26021933  0.6997619  -0.76020959 -0.59827126
 -1.01171539 -0.78448996  0.78650988 -0.42980558 -0.97198181  0.15120106
  0.05755798 -0.15073403  1.61991212  0.35661037  0.88236813  0.29541979
  0.02679199 -0.68353987 -1.16650421 -0.01435054 -0.91160572 -0.46150978
 -0.83208709 -1.01961641 -1.08450051  0.70219264  0.88012206 -0.46119885
 -0.93934185 -0.97595407  0.13837714 -0.3329847  -0.52398475 -0.728972
  0.64843618  0.67827483 -0.15073403 -1.47774518 -1.03528711  0.04945057
  0.31748845 -1.19748279 -0.50440305  1.22571548  0.22787307  0.17735199
 -0.0149273   0.24431197  0.02000566 -0.09660463  0.99609044 -0.47580736
  0.6997619   1.1893491   0.15120106  0.04593322 -1.22820412 -0.54885046
  0.2804227  -0.06956287 -0.25104155  0.0105516   0.11724113 -0.02231782
  0.15018819 -0.30205783  0.21685968  0.35595218 -0.47245044  0.38610221
 -0.27771028  0.24001584  0.1153761  -0.55854815  0.72956771  0.50592523
 -0.97198181 -0.96062348  1.65399894  0.61177425  1.69498741 -0.63585456
  1.71006213  0.08646988 -0.72726233 -0.25574532  0.39951935 -0.76530691
  0.00529335  0.55597364 -0.09401419 -0.5207211  -0.47984801  0.12319855
 -0.7642502   0.00314864 -0.53703918  0.91001345 -0.86184843 -0.42871784
 -0.60262266 -0.80768756 -1.07755892 -0.41799442 -0.84568561 -0.83589464
 -0.59961816 -0.53960359  0.03770168 -0.57495951 -0.12952572 -0.09100957
 -0.15876898]
Pearson r: 0.705
MSE: 0.558
Fold 2, k_feat 128
X_train.shape:  (265, 128)
y_pred:  [ 1.62194572  1.36837075  1.21759195  1.33378783  1.07190533  1.17592578
 -0.18641565  0.94019747  1.38796142  1.26621863  0.57168608  0.35038125
 -1.5017543   2.35830112 -1.34022031 -0.4015791  -0.69839811  0.2067936
 -1.01701621  0.06858829 -0.50127386 -0.09428286  2.33772623  0.61839507
  0.73529789 -0.52475843 -0.10349245  0.06687372  0.15710177 -0.36327659
 -0.4534944  -0.82465828 -0.05249197 -0.25281639 -0.46916457 -0.08551561
 -0.33262213 -1.19777279  2.35830112  1.60100713  0.56399642  1.43999855
  0.15701886  0.21739281 -0.02877867  0.58202551 -1.96614428 -1.8963142
 -0.58987066 -0.70971631 -0.59184489 -1.76663015 -1.017255   -0.58987066
  1.16573913  1.42833679  0.57423185  0.09196899  1.33378783  0.12891021
  0.03336181  0.44402808  1.04753216  1.51170166  0.17255382  1.06647077
  0.91293823  0.3906791   0.47951466  1.16878362  1.24667217  0.19983116
 -0.06644736  1.73277251  0.08168131  0.01153979  1.81714219  1.24667217
  0.64507483  0.10625679 -0.64685712  0.27035218 -0.42448169  0.44096266
 -0.10411633 -0.66413983 -0.28929007 -0.11502715 -0.40743993  0.16745567
  0.26280253  0.14995276 -0.09233227 -0.2796262  -0.21229012  0.25033317
 -0.1908012   1.05769289  1.10924157 -0.14931859  2.60085845  1.49059388
 -1.017255   -0.47355494 -1.017255   -0.78761145 -0.5020013  -0.38385165
  1.33378783 -0.86694592  2.35830112 -0.88409166  0.28781999 -0.13341971
 -0.43840623 -1.2008723  -1.27475478  0.35757215 -0.47072332 -0.79914217
 -0.64924686 -0.07637117 -0.96233855 -0.44266654  0.11765639 -0.61973542
 -0.7890106  -0.42676777  1.10836852 -0.20418488  0.48195673  0.46527845]
Pearson r: 0.664
MSE: 0.693
Fold 3, k_feat 128
X_train.shape:  (265, 128)
y_pred:  [ 1.33394005  0.79323023  2.43473385  1.72282228 -0.0297309   0.53815627
  0.17420655 -0.17457417 -1.00418622  0.12845314  0.50690952  1.3655111
 -1.13233969 -0.31186085  0.1167916  -0.31186085  0.1167916  -0.81563789
  0.83077025  0.77035162 -0.49707015 -0.82506537 -0.4453176  -0.29801433
 -0.1082878  -0.8548205  -0.89214739 -0.44355013 -0.54047567  0.23806145
  0.39711402  0.0653089   1.06076909 -0.51455031 -0.25146638 -0.84612976
  0.0653089  -0.83419811 -0.67452148  0.57366395 -0.56699007  0.58904598
  0.55074712 -0.26207236  1.33974189 -0.59468302  0.68225929  1.07938535
  0.33155986  0.416922   -0.07109373  0.45777412  0.66383367 -0.04506303
  0.68391904  0.04608606 -0.4017158   0.18911743  0.18911743 -0.6921979
  0.709787   -0.34397304  1.33974189  0.00871313  0.20045995  0.267262
 -0.23054936 -0.00429166 -0.39169911 -0.22959201  1.00917202  0.88265124
  0.01723421 -0.11550563 -0.83213595  0.23522334 -0.03198474 -0.27868081
  0.20753025  0.14684147  0.20753025  0.17335593  0.19014848 -0.43667593
  0.61045384 -0.06733755  0.16127715 -0.1200721   0.18189957  0.18322528
 -0.1495327   0.22609045 -0.44266614  0.19714763 -0.83331428 -0.93259666
 -0.62650042 -0.93613205  0.10667682 -0.13833764  1.33974189  0.79204443
  0.67305336  0.05330395  0.75520382 -0.88282794  0.76125212 -0.51455031
  0.13692319  0.02840968 -0.65890731 -0.81436131 -0.34059719 -0.6818865
 -0.68267221 -0.64541434 -0.59321007 -0.35074891 -0.49167876 -0.74905692
 -0.7812574  -0.50541731  0.63420981 -0.12525217 -0.91629509 -0.3999482
 -0.59026405  1.06670116 -0.38993158 -0.69367079 -0.26633414 -0.60786681]
Pearson r: 0.659
MSE: 0.452
Average pearson_r accross all folds = 0.6758929425143586
Average mse accross all folds = 0.5677705959123821
Fold 1, k_feat 256
X_train.shape:  (264, 256)
y_pred:  [ 0.89219819  1.67768191  1.72113986  0.9956035   1.80264263  1.97893552
  2.00846359  1.96549246  1.97893552  0.92361718  0.1808043   1.06581806
 -0.78818357 -1.01011052 -1.51254385  0.37584523  1.16813049  1.2107129
  0.56429127 -0.10080059  0.21243045  2.00846359 -0.54356659 -0.59780498
 -1.10366767 -0.85749674  0.5529098  -0.39406104 -1.02751633  0.19152719
 -0.04718372 -0.47891554  1.38283096  0.55119014  0.95159088  0.17785147
 -0.1886077  -0.69618004 -1.39213139 -0.04407539 -0.68747711 -0.44596865
 -0.72943814 -0.85345608 -1.01788103  0.56784495  0.62056544 -0.60604196
 -1.02782711 -0.92898588  0.2048931  -0.41131181 -0.53579597 -0.65546258
  0.60056957  0.68076148 -0.54791807 -1.52951484 -1.02596242  0.2761746
  0.2219884  -1.42955433 -0.46648268  1.16762279  0.05418573  0.36740115
  0.05680181  0.15345207 -0.22248734 -0.24611003  0.88815748 -0.47052338
  0.29842446  0.91786693  0.19152719 -0.09890993 -1.16365667 -0.34541749
  0.37584495  0.11382212 -0.04842694  0.26923314  0.24716483  0.24498912
  0.04108972 -0.32132878  0.25275953  0.23286668 -0.24321923  0.20737945
 -0.38753411  0.10915977  0.03362994 -0.58148681  0.58699666  0.79545476
 -1.05207179 -0.92494527  1.36822227  0.61339071  1.14761651 -0.7316137
  2.05085463  0.0493266  -0.66779172 -0.35738415  0.51245138 -0.764561
  0.12060838  0.4271305  -0.24460747 -0.63432653 -0.53020119 -0.11167935
 -0.485132   -0.0506546  -0.49466375  0.69739047 -0.65763826 -0.49259169
 -0.68219315 -0.7188702  -0.96659543 -0.40369666 -0.7300598  -0.81242754
 -0.7288165  -0.49725407  0.17013265 -0.54325571 -0.12874865 -0.30765258
 -0.19466886]
Pearson r: 0.714
MSE: 0.553
Fold 2, k_feat 256
X_train.shape:  (265, 256)
y_pred:  [ 1.87517794e+00  1.58271337e+00  1.77407000e+00  1.30869284e+00
  1.33633425e+00  1.51288338e+00 -3.25140327e-01  1.27907764e+00
  5.81401947e-01  1.38735584e+00  5.83901103e-01  8.39606635e-01
 -1.42184455e+00  2.59774098e+00 -1.09587074e+00 -1.11597870e-01
 -8.46267112e-01  9.04053720e-01 -9.01805499e-01  2.14794824e-01
 -4.07439963e-01 -1.47759736e-01  2.59119451e+00  1.34789452e+00
  9.96536561e-01 -4.35184872e-01 -7.73062181e-02  9.82400766e-02
  3.79373004e-01 -6.98245803e-02 -5.16642770e-01 -8.04960879e-01
  2.18306915e-01 -4.50460074e-01 -7.02034952e-01 -3.45559337e-01
 -4.30404843e-01 -1.11019868e+00  2.58714179e+00  1.77994118e+00
  7.94819717e-01  1.07364056e+00  3.12278186e-03  3.73782717e-01
  7.13161610e-02  6.18499182e-01 -1.43711999e+00 -1.45738292e+00
 -3.48520906e-01 -4.32093347e-01 -6.49974151e-01 -1.38911190e+00
 -8.13964881e-01 -2.98642366e-01  1.28562418e+00  6.15210053e-01
  3.69620620e-01 -3.54132161e-01  1.33924346e+00 -1.18612099e-01
 -1.46512664e-01  5.58021301e-01  1.46856422e+00  9.49728905e-01
  3.71288411e-01  1.47157782e+00  5.84389258e-01  4.28259452e-01
  7.94507956e-01  1.63622868e+00  1.11697230e+00  3.17072510e-04
 -3.39743448e-02  1.60256073e+00 -3.11969316e-01 -2.20707204e-01
  1.87917873e+00  1.06647013e+00  1.05306566e+00  2.91794576e-01
 -6.20982580e-01  2.70611979e-01 -3.46338714e-01  4.87879739e-01
 -8.38531605e-02 -5.55230908e-01 -2.21018867e-01 -2.95213240e-01
 -4.40172820e-01  2.08248171e-01  2.28199352e-01 -2.64925673e-02
 -9.25816191e-02 -3.71589737e-01 -3.58496669e-01  1.94843252e-01
 -1.56120049e-01  1.28094811e+00  1.49230850e+00  6.49721213e-02
  1.92708279e+00  4.55250771e-01 -8.90534833e-01 -3.96466672e-01
 -9.21968196e-01 -7.87944071e-01 -5.85132167e-01 -3.21711281e-01
  1.24281162e+00 -6.66185021e-01  1.75011791e+00 -7.18946910e-01
  3.82589254e-01  8.48611648e-02 -5.07508757e-01 -8.25172616e-01
 -1.03601328e+00 -1.72231442e-01 -1.47136351e-01 -7.31338503e-01
 -7.11698911e-01 -5.43047119e-01 -8.45124015e-01 -5.08132121e-01
 -2.45958173e-01 -8.09585539e-01 -5.93237420e-01 -7.48640129e-01
  9.01247814e-01 -4.57318408e-01  1.74580204e-01  3.01043032e-01]
Pearson r: 0.741
MSE: 0.543
Fold 3, k_feat 256
X_train.shape:  (265, 256)
y_pred:  [ 2.02448632e+00  9.41984176e-01  2.34706078e+00  1.95552881e+00
  2.02522056e-01  9.12818043e-01  8.29032107e-01  1.94715049e-01
 -1.03629836e+00  1.75604937e-01 -1.26455075e-01  1.12405109e+00
 -1.09875454e+00 -2.96718061e-01  1.35351768e-01 -2.92593577e-01
  1.35351768e-01 -7.81934902e-01  1.06972571e+00  6.06231184e-01
 -6.67058084e-01 -6.91019372e-01 -4.74778309e-01 -2.97719732e-01
 -3.95823754e-01 -7.83525682e-01 -8.87119410e-01 -5.60214202e-01
 -4.52977461e-01  8.42571873e-02  7.28589299e-01  7.00915965e-01
  1.13318405e+00 -4.71537769e-01 -2.35852284e-01 -8.25065383e-01
 -4.01160185e-02 -8.10629606e-01 -6.63031786e-01  5.80084715e-01
 -5.26628974e-01  6.78606369e-01  6.80374003e-01 -1.38926804e-01
  1.79837436e-01 -4.85973259e-01  6.67705721e-01  9.94620502e-01
  5.13921342e-01  7.13222443e-01 -5.05449100e-02  2.69103100e-01
  7.31635221e-01  1.30901549e-02  9.00189521e-01  9.09839686e-02
 -2.45574364e-01  5.41319550e-01  5.27178433e-01 -5.30164311e-01
  6.80373761e-01 -5.21326173e-01  2.88314624e+00  1.34762600e-01
  1.78659022e-01  3.19775549e-01  1.09671926e-01  6.99492451e-02
 -2.80043283e-01 -2.66196863e-01  7.86137624e-01  1.86848957e-01
  5.41656207e-01 -2.45279899e-01 -8.53524416e-01  2.35517939e-01
 -1.34567613e-03 -6.49807260e-02  2.01048878e-01  1.53028134e-01
  2.26090461e-01  1.36235441e-01  2.25795961e-01 -4.28525108e-01
  8.28089482e-01 -5.05449542e-02  2.37580274e-01 -6.96943439e-02
  1.92210740e-01  1.65205217e-01 -2.34968613e-01  2.22849805e-01
 -3.50712149e-01 -1.54866452e-02 -8.82808160e-01 -9.22580034e-01
 -6.88367796e-01 -9.10678079e-01  2.09003516e-01  3.36804879e-03
  4.76211539e-01  8.27695140e-01  9.31078802e-01  4.50286161e-01
  1.07676684e+00 -9.16687971e-01  1.47317304e+00 -4.75073050e-01
  1.49787616e-01  2.37064698e-01 -6.10150001e-01 -9.03430687e-01
  5.28620173e-02 -7.33737297e-01 -6.55961224e-01 -5.73471393e-01
 -2.36147012e-01 -2.85346269e-01 -6.73097415e-01 -6.30330525e-01
 -6.74226806e-01 -5.42316756e-01  5.91785659e-01 -9.00539125e-03
 -9.56754364e-01 -3.02138911e-01 -6.93376350e-01  3.01450967e-01
  1.14140098e-01 -5.60066767e-01 -3.04689166e-01 -4.65350852e-01]
Pearson r: 0.705
MSE: 0.405
Average pearson_r accross all folds = 0.7200319123333149
Average mse accross all folds = 0.49999448604395086
Fold 1, k_feat 512
X_train.shape:  (264, 512)
y_pred:  [ 1.18304996  2.0347177   1.11649764  1.17380283  1.27742574  0.99976844
  1.25391725  1.12399366  1.41181496  1.13176425  0.12283606  0.75515144
 -0.81926577 -0.92836432 -1.31285098  0.4812139   0.82964522  1.16750865
  0.15912444 -0.13701128  0.27109815  1.24055188 -0.71296463 -0.61971799
 -0.95012168 -0.85438868  0.61774607 -0.41581862 -0.91189065  0.1909057
  0.10465286 -0.47300995  1.39181886  0.36538075  1.31312881  0.10884892
 -0.15597139 -0.52087634 -1.34113576 -0.06303562 -0.65142178 -0.44938762
 -0.74839828 -0.76207445 -0.92960759  0.47655132  0.73297946 -0.53983692
 -0.85034789 -0.97405501  0.20225101 -0.34743788 -0.52211984 -0.75803383
  0.57166302  0.62170535 -0.60573096 -1.57736069 -0.97467682  0.54735659
  0.30186965 -1.47510024 -0.5319107   0.79141413  0.22043407  0.28259848
  0.23418801  0.20928083 -0.15597141 -0.15534999  0.45355051 -0.55226965
  0.83611922  0.76149757  0.1909057  -0.23865031 -0.97623083 -0.40400745
  0.1946358   0.04326548 -0.05029191  0.03394089  0.27886872  0.15764808
  0.01560234 -0.43135984  0.17256743  0.21079849 -0.03039937  0.1887301
 -0.50844378  0.22509627  0.2512054  -0.44534673  0.66589367  0.8097526
 -1.12480397 -0.80931943  1.39161146  0.69837467  0.98447621 -0.68872027
  1.32462933  0.07745595 -0.61878541  0.09392943  0.14335014 -0.47891548
 -0.20539222  0.54379253 -0.25232624 -0.4332247  -0.50191637  0.07621259
 -0.30796343 -0.24828563 -0.42203504  0.48960597 -0.80248132 -0.29304398
 -0.56656736 -0.6190963  -0.9544734  -0.626556   -0.73378969 -0.62251537
 -0.75088487 -0.29584134  0.47313231 -0.48419947 -0.39095293 -0.45529306
 -0.20228402]
Pearson r: 0.753
MSE: 0.485
Fold 2, k_feat 512
X_train.shape:  (265, 512)
y_pred:  [ 1.66199925  1.54016062  1.33337244  1.48607343  1.43510416  1.37758786
 -0.45544796  1.2569442   0.18159441  0.80871295  0.60987431  0.7064101
 -1.43337885  1.53969286 -1.08828175 -0.14557762 -0.81332647  0.34905123
 -0.817379    0.06297694 -0.51436706 -0.11876789  1.64350283  1.09998277
  1.24946231 -0.41086902 -0.03085686 -0.08977602  0.30429548 -0.04956142
 -0.57079205 -0.68738309  0.35183589 -0.41741551 -0.71419287 -0.5439824
 -0.44173136 -1.07456509  2.58963571  1.5169359   0.66484456  1.01269518
 -0.08177462  0.21167747 -0.0324157   0.67367735 -1.40251665 -1.44335451
 -0.60227797 -0.27837922 -0.48568674 -1.44023735 -0.92181205 -0.51062614
  1.32240962  0.69487547  0.52973066 -0.39076177  1.47921514 -0.31017681
 -0.03054516  0.48850305  1.17916485  0.74527375  0.25532095  1.21231344
  0.44454755  0.37565319  0.67637901  1.33261916  1.45334061  0.15751238
  0.07310859  1.6223043  -0.2353591  -0.25468697  1.70663023  1.02033249
  1.23543396  0.16242223 -0.57048052  0.30770914 -0.29147239  0.43332512
 -0.28835508 -0.37096619 -0.41180422 -0.20667894 -0.40806351  0.31299298
  0.16647487 -0.01059389  0.08012267 -0.38592993  0.08913722  0.38562865
  0.20824783  1.06241792  1.23044622 -0.09756955  1.71925565  0.32608619
 -0.87598612 -0.53213627 -0.72603882 -0.80740332 -0.51748441 -0.19545614
  1.46861595 -0.70421745  1.09702106 -0.87255723  0.18455588 -0.0402092
 -0.56549258 -0.80802681 -0.85884052 -0.13282223 -0.27017003 -0.6958001
 -0.58980826 -0.46978807 -0.91744782 -0.2001322  -0.43300265 -0.78371103
 -0.53992975 -0.55458155  0.87817913 -0.31578807  0.34759639  0.58036281]
Pearson r: 0.769
MSE: 0.470
Fold 3, k_feat 512
X_train.shape:  (265, 512)
y_pred:  [ 1.62301627  0.97713571  1.63529162  1.73869844  0.35925269  1.27897466
  1.18709688  0.17836435 -1.00153477  0.33804103  0.0449077   1.75657111
 -0.97914436  0.02457975  0.13859241  0.02457975  0.13800322 -0.80679969
  0.96128108  0.56385698 -0.29713054 -0.76702777 -0.35045435 -0.14658658
 -0.281811   -0.8439202  -0.95233548 -0.64123098 -0.43824715 -0.05587728
  0.86641779  0.89514186  0.78093278 -0.53193209 -0.26766979 -0.93642669
 -0.07487958 -0.78706111 -0.57376596  0.87776017 -0.30037122  0.62380956
  0.61173068 -0.1096429   0.08756662 -0.61265405  0.78083466  1.08724979
  0.55899606  0.53616393 -0.11535837  0.41493333  0.79468117  0.19525521
  0.93027377  0.15833106 -0.28004325  0.57284245  0.54190876 -0.50158748
  1.01681432 -0.37873664  2.69341963  0.12887053  0.13535189  0.18720262
 -0.14378786  0.03530346 -0.32217204 -0.28755585  0.79232434  0.10870464
  0.50802898 -0.37343364 -0.86071279  0.16864227 -0.14069449 -0.12702478
  0.19221082  0.12062141  0.16923155  0.20782487  0.31447252 -0.4789029
  0.47621148 -0.12949952  0.49742314 -0.11889369  0.07525197 -0.10239567
 -0.3433838   0.18189949 -0.30950401 -0.05702615 -0.87426466 -0.93377515
 -0.76143026 -0.88251376  0.16923168  0.11679157  0.06983132  0.74270766
  0.67315594  0.11058386  1.09481152 -0.82860058  0.87599246 -0.53517277
  0.23316131  0.45912423 -0.36901456 -0.9723686   0.07554675 -0.59497768
 -0.75789506 -0.48597329 -0.21581913 -0.20874855 -0.62679515 -0.61677865
 -0.45828028 -0.201678    0.66323744 -0.28917617 -0.97590386 -0.54636769
 -0.69190329  0.58521588  0.12592442 -0.62384909 -0.24999338 -0.60587818]
Pearson r: 0.718
MSE: 0.385
Average pearson_r accross all folds = 0.746897965073615
Average mse accross all folds = 0.44688060834945403
Fold 1, k_feat 1024
X_train.shape:  (264, 1024)
y_pred:  [ 1.07581636  1.86250683  1.22034853  1.14248753  1.05841014  0.919079
  0.92444586  0.87553282  1.0626374   0.91753015  0.3652772   1.01396262
 -0.92649938 -0.92245869 -1.24447019  0.44578015  0.92164853  0.84953765
  0.04792785 -0.3573842   0.20023074  0.97573149 -0.81242761 -0.5479181
 -0.95664902 -0.81646839  0.71689446 -0.36173555 -0.88640331  0.23255587
  0.02710278 -0.42421085  0.98692118  0.24343481  0.89010002 -0.02449376
 -0.16995838 -0.53672829 -1.28332292 -0.14882256 -0.58708157 -0.42265693
 -0.8307661  -0.96908195 -1.01694857  0.49986303  0.9677279  -0.60355535
 -0.7580338  -0.93799967  0.18033807 -0.29677385 -0.62096121 -0.70736983
  0.46007794  0.59310966 -0.57620287 -1.55467064 -0.92867512  0.58129832
  0.22323163 -1.48753312 -0.44441432  1.17092786  0.21017689  0.13495801
  0.09626069  0.0327753  -0.11649699 -0.25139392  0.38330471 -0.57993275
  0.7607721   0.78499046  0.24156969 -0.07919853 -0.97312272 -0.46399611
  0.19401417  0.14055272  0.03052181  0.27109806  0.23348864  0.30062622
  0.01622396 -0.58956822  0.10884889  0.36993946 -0.15597158  0.24592141
 -0.56004022  0.27731446  0.24965128 -0.43384637  0.81130652  0.98039389
 -1.117655   -0.83356353  1.20605069  1.08980326  0.91325641 -0.70457221
  0.95602545 -0.0216963  -0.45218483  0.41998178  0.19339256 -0.52709297
 -0.27594873  0.54089164 -0.25574525 -0.31697724 -0.50999774 -0.09038805
 -0.49134842 -0.31604481 -0.42141344  0.30373437 -0.74746581 -0.43913041
 -0.40556154 -0.69120697 -0.96814959 -0.47891549 -0.59391968 -0.84537475
 -0.64365124 -0.31759888  0.45386133 -0.32505858 -0.27594881 -0.38784462
 -0.26569174]
Pearson r: 0.757
MSE: 0.484
Fold 2, k_feat 1024
X_train.shape:  (265, 1024)
y_pred:  [ 1.59549456  1.59393601  1.46051078  1.36355932  1.5992355   1.43494821
 -0.39917877  1.04683087  0.43457209  0.66968687  0.90935342  0.81209029
 -1.45520071  1.13037709 -0.92337079 -0.24003519 -0.70328204  0.15727868
 -0.85011161  0.14262675 -0.4192861   0.00262395  1.19615459  0.81193425
  1.19989565 -0.53930629 -0.08634677  0.13226137  0.14270466  0.01341021
 -0.4333144  -0.77467046  0.95891996 -0.40588113 -0.85541146 -0.67865435
 -0.35724963 -0.95111569  1.77599221  1.64100883  0.69588875  1.16186308
 -0.07047925  0.13498926  0.15693571  0.63439793 -1.43618472 -1.49073914
 -0.55614035 -0.22039535 -0.55707533 -1.62946405 -0.91807112 -0.51218488
  0.73883087  0.80510691  1.08174572 -0.29833063  1.40127999 -0.29186205
  0.02775038  0.4609608   0.90187134  0.87880268  0.26373796  1.22358805
  0.39276751  0.44766532  0.67500217  1.15718704  1.1827496   0.20263683
  0.03679079  1.42029622 -0.34657259 -0.30799461  1.42707664  1.04745402
  0.982924    0.23331202 -0.63376384  0.29305732 -0.33979217  0.09898306
 -0.20075589 -0.26653299 -0.40494592 -0.34501387 -0.46168295  0.3251509
  0.21697692 -0.05688735 -0.05610809 -0.43082051 -0.12687319  0.24378656
  0.57002312  0.93678631  1.12383073 -0.03015557  1.85049864  0.62598078
 -0.72416831 -0.47711397 -0.7699942  -0.83670682 -0.34602697 -0.46137117
  1.3944217  -0.56175187  1.63789129 -0.8229904   0.00405792 -0.05143185
 -0.60882452 -0.87255721 -0.95548027 -0.15418165 -0.31329417 -0.75440746
 -0.67304303 -0.5654925  -0.90497823 -0.31641157 -0.37938327 -0.66743171
 -0.43175572 -0.5365006   0.47042209 -0.31360589 -0.12126191  0.37409427]
Pearson r: 0.752
MSE: 0.495
Fold 3, k_feat 1024
X_train.shape:  (265, 1024)
y_pred:  [ 1.51499381  1.04097205  1.24778595  1.74625999  0.49182564  1.26369434
  1.15881458  0.23728561 -1.03924444  0.3374518   0.12975443  1.41777366
 -1.0033021  -0.07705953  0.20370037 -0.08088941  0.19633521 -0.84126868
  0.63765587  0.63264755 -0.36547922 -0.95528136 -0.40790261 -0.30302265
 -0.06586446 -0.85747211 -1.18713688 -0.63769574 -0.49687385  0.0832065
  0.82973927  0.82325794  0.51745628 -0.48243808 -0.30626323 -0.80738899
 -0.03227943 -0.82477073 -0.55284894  0.76551504 -0.53252107  0.68243626
  0.67241964  0.01868768  0.17718603 -0.65772883  0.81235751  0.92754849
  0.60377629  0.77582636  0.0375424   0.52776769  0.86774354  0.1341734
  0.74341962  0.4620704  -0.23290634  0.51185886  0.51185886 -0.46829693
  1.22922562 -0.35251663  2.44035273  0.2888418   0.06700321  0.21636864
 -0.19313436  0.14094935 -0.40584028 -0.31657457  0.96702599  0.17453446
  0.65179691 -0.31097705 -0.87455935  0.15273354 -0.13480245 -0.29182773
  0.16982076  0.15862567  0.16893693  0.21636845  0.18985409 -0.4956955
  0.46943553  0.01102777  0.25407824  0.03577477  0.18101568 -0.08972763
 -0.35015981  0.17306136 -0.45179901 -0.03699291 -0.91757185 -0.89135192
 -0.80385361 -0.84627712  0.07878746  0.03989927  0.09616915  0.80911683
  0.6703572   0.30681284  0.78790515 -0.77262546  0.92283493 -0.48744639
  0.17217771  0.5136265  -0.38698548 -0.85570436  0.01397392 -0.64947986
 -0.6771729  -0.65920188 -0.1857693  -0.35781946 -0.30537947 -0.57671214
 -0.53605639 -0.14157827  0.35365518 -0.14069451 -0.97767149 -0.49687378
 -0.69514395  0.57726152 -0.18989377 -0.6323927  -0.15247857 -0.54135936]
Pearson r: 0.733
MSE: 0.367
Average pearson_r accross all folds = 0.7472589076030065
Average mse accross all folds = 0.4485307794112792
Fold 1, k_feat 2048
X_train.shape:  (264, 2048)
y_pred:  [ 0.99469177  2.08692047  1.5495092   1.49200709  1.2722557   1.34933964
  1.53334627  1.36985391  1.34778554  0.75753448  0.38050746  1.12368288
 -0.87863283 -0.84040168 -1.33429768  0.43800956  0.73484447  0.63662456
  0.1399311  -0.38038502  0.07341531  1.5662934  -0.52211984 -0.48451039
 -0.93209417 -0.88857915  0.75877789 -0.28527333 -0.90816082  0.32891066
  0.00254787 -0.50688957  0.60958319  0.51198511  0.86974115  0.01466988
 -0.09318535 -0.54418803 -1.34766308 -0.02884516 -0.66416548 -0.4195487
 -0.83542848 -0.91220151 -0.97219022  0.40537315  0.73639856 -0.54543165
 -0.75368223 -1.03808441  0.15267487 -0.28278685 -0.59671703 -0.65017851
  0.62543519  0.58129847 -0.54667476 -1.46826211 -0.85718607  0.7736973
  0.34880379 -1.48970888 -0.55786427  0.95179824  0.18406785  0.1138221
  0.2216774  -0.0216963  -0.06707634 -0.1164971   0.30435597 -0.57806782
  0.65838226  0.6994108   0.33916779 -0.30205797 -0.87024056 -0.43788706
  0.25307037  0.05227932  0.07559098  0.18966272  0.25990852  0.17349999
  0.04544122 -0.52616054  0.08833466  0.11693016 -0.09380718  0.17381075
 -0.59391982  0.20582543  0.16541852 -0.48575366  0.78923816  1.1233721
 -1.03342221 -0.84444233  1.45128931  0.8240505   0.91045902 -0.57589184
  0.99034024 -0.00335779 -0.42141345  0.28943656  0.06222561 -0.41177795
 -0.16591779  0.45075316 -0.36857369 -0.18363461 -0.28620587  0.00938586
 -0.49134845 -0.3054769  -0.34091061  0.15080993 -0.83822587 -0.44845504
 -0.43167062 -0.55444532 -0.99394781 -0.23523109 -0.48264542 -0.84195568
 -0.66944946 -0.37789845  0.30000445 -0.34868108 -0.43788712 -0.22062241
 -0.28589518]
Pearson r: 0.757
MSE: 0.480
Fold 2, k_feat 2048
X_train.shape:  (265, 2048)
y_pred:  [ 1.35670115  1.40252715  1.32428007  1.30245812  1.33550286  1.246345
 -0.2431525   1.07457574  0.5910658   0.70765694  0.81926032  0.87287973
 -1.43369066  1.01908576 -0.93615221 -0.27120921 -0.56206342  0.34447904
 -0.84262984  0.26373808 -0.42957354  0.15307007  1.16809796  0.71576228
  1.00755149 -0.40525773 -0.08354108  0.18393238  0.26560852  0.07949924
 -0.62597022 -0.75222517  0.61881074 -0.38842368 -0.74037908 -0.53525363
 -0.53525368 -0.95766232  2.29098801  1.56182671  0.67367726  0.92930454
  0.16772195  0.19671387  0.22601745  0.57485548 -1.41342756 -1.44740715
 -0.54772335 -0.19202696 -0.60134253 -1.46860578 -0.90778371 -0.55177598
  0.82798871  0.6347095   0.77125189 -0.34976785  1.35420709 -0.36286096
  0.0517544   0.50471364  0.88347869  0.92369342  0.2840011   1.06428849
  0.48600917  0.3899932   0.6219283   1.0960858   1.57616648 -0.03210392
  0.04302558  1.43775384 -0.278691   -0.29116058  1.34766071  1.17838515
  1.13255957  0.28181889 -0.61069505  0.26841427 -0.44921316  0.19172597
 -0.12780853 -0.24221724 -0.3725249  -0.17269912 -0.42988536  0.34042613
  0.21261251 -0.07013637 -0.03927401 -0.46074766 -0.02898658  0.20887162
  0.70453928  0.92930457  1.08548667  0.11160858  1.62542181  0.74350711
 -0.79680396 -0.62752894 -0.7740469  -0.74318454 -0.30113624 -0.54616459
  1.36979411 -0.64498668  1.10356777 -0.77092972 -0.12718493  0.05643038
 -0.64779212 -0.81301473 -0.93303492 -0.22164234 -0.48537508 -0.70109974
 -0.54678805 -0.50719699 -0.89469082 -0.29490146 -0.46760591 -0.68894185
 -0.36504321 -0.51000265  0.59293619 -0.14495419 -0.12874364  0.43924811]
Pearson r: 0.768
MSE: 0.469
Fold 3, k_feat 2048
X_train.shape:  (265, 2048)
y_pred:  [ 1.4428152   1.35443341  1.45489429  1.66259175  0.58344824  1.28107618
  1.2539725   0.26556788 -0.99741031  0.29031482  0.51716182  1.19711347
 -0.86748854 -0.03434154  0.17423968 -0.03610918  0.18808616 -0.77674993
  0.66800036  0.59022423 -0.18193941 -1.01862177 -0.3734337  -0.21493526
  0.00808175 -0.84362567 -1.12114493 -0.52103151 -0.52603991  0.02546364
  0.96938274  0.69097958  0.54456011 -0.44472845 -0.20668622 -0.79619401
  0.00572477 -0.81387029 -0.57553365  0.60996278 -0.50217666  0.63912909
  0.63088009  0.07053845  0.39342707 -0.62443834  0.96790969  0.88866043
  0.58521605  0.81029529  0.09882053  0.46295428  0.85448626  0.13328955
  0.69422033  0.4055059  -0.37844193  0.60318692  0.5961163  -0.45533424
  1.31760756 -0.47153768  2.56703346  0.28118201  0.18131052  0.21077114
 -0.30096042  0.1123725  -0.38845852 -0.23143333  0.92813794  0.36720707
  0.65268071 -0.39287767 -0.85482067  0.2187254  -0.04612583 -0.30272814
  0.16805312  0.15597427  0.17099917  0.22667972  0.16304488 -0.53281585
  0.55987973  0.03135569  0.39431087  0.01485774  0.1553849   0.02840967
 -0.28446255  0.15803644 -0.30390657  0.02664216 -0.8895843  -0.8686672
 -0.76260871 -0.82388701  0.18897025  0.02074987  0.21460093  0.88188457
  0.55899595  0.28412807  0.91429129 -0.75406516  0.85684299 -0.45386127
  0.32920299  0.61408732 -0.34927589 -0.75936813 -0.02373571 -0.59556686
 -0.74522702 -0.58820175 -0.19549123 -0.05201796 -0.22966567 -0.56964152
 -0.49775755 -0.14452436  0.60200838 -0.28652468 -0.88045132 -0.4571019
 -0.62708982  0.35807427  0.01073328 -0.6138325  -0.07205107 -0.52515598]
Pearson r: 0.742
MSE: 0.359
Average pearson_r accross all folds = 0.7558374417213104
Average mse accross all folds = 0.4361409578070625
Fold 1, k_feat 4096
X_train.shape:  (264, 4096)
y_pred:  [ 9.90961965e-01  1.76459800e+00  1.38477342e+00  1.39658466e+00
  1.25422799e+00  1.40963911e+00  1.59271323e+00  1.39316561e+00
  1.42704519e+00  9.47136006e-01  4.41428671e-01  1.08358689e+00
 -8.87957372e-01 -9.06606759e-01 -1.31906743e+00  5.33431933e-01
  8.72538700e-01  6.25745819e-01  1.08227282e-01 -3.58938273e-01
  1.31158388e-02  1.62410627e+00 -4.91348458e-01 -4.41616982e-01
 -9.37067283e-01 -8.90133189e-01  9.14499708e-01 -1.41051923e-01
 -8.92930532e-01  3.51289802e-01  7.27936555e-02 -5.88014134e-01
  7.87062544e-01  4.55415438e-01  1.02546301e+00 -3.97949342e-03
 -1.49755013e-01 -5.57242516e-01 -1.36351501e+00 -7.76443108e-02
 -6.71936032e-01 -2.66935068e-01 -9.36756420e-01 -9.28053399e-01
 -9.18417970e-01  3.26113499e-01  5.74771045e-01 -4.77361643e-01
 -6.96490931e-01 -9.91150251e-01  2.38150903e-01 -2.34609450e-01
 -6.23447832e-01 -5.47918084e-01  5.69487283e-01  6.47503595e-01
 -5.75892091e-01 -1.52172348e+00 -8.87646604e-01  6.22016037e-01
  3.26113828e-01 -1.46888380e+00 -3.99655923e-01  9.71380045e-01
  1.97744018e-01  2.13285130e-01  2.03028094e-01 -3.41291789e-02
 -4.34538533e-02 -1.94202641e-01  3.65587911e-01 -5.19011666e-01
  8.14104113e-01  6.02434270e-01  3.17721033e-01 -1.85188856e-01
 -8.47239704e-01 -4.53739080e-01  1.46458387e-01  9.61051409e-02
  5.69416144e-02  4.60628298e-02  2.42813203e-01  1.54539791e-01
  3.58057865e-02 -5.21187434e-01  9.08212482e-02  1.23768356e-01
 -8.97666052e-02  1.21281891e-01 -4.73320849e-01  1.47080069e-01
  9.42402381e-02 -4.45036008e-01  6.81072107e-01  9.06418275e-01
 -1.05922045e+00 -8.36671698e-01  1.14854861e+00  8.67254695e-01
  1.05374802e+00 -6.27799125e-01  9.21959319e-01 -1.29933043e-02
 -4.97875713e-01  1.98987303e-01  3.98464664e-02 -5.05646284e-01
 -2.49645151e-04  6.39111328e-01 -2.41447488e-01 -2.07567925e-01
 -2.82786886e-01 -1.46646797e-01 -4.63685280e-01 -1.54728220e-01
 -4.38508665e-01  2.90679838e-01 -5.31444498e-01 -2.67867443e-01
 -3.92196183e-01 -4.31981439e-01 -8.76146264e-01 -3.78520055e-01
 -5.73094618e-01 -9.78406592e-01 -5.43255743e-01 -1.72445023e-01
  2.87260717e-01 -5.06578732e-01 -4.89794321e-01 -5.14349247e-01
 -2.04459777e-01]
Pearson r: 0.754
MSE: 0.487
Fold 2, k_feat 4096
X_train.shape:  (265, 4096)
y_pred:  [ 0.95081467  1.29591173  1.10294413  1.31711     1.5437457   1.16123978
 -0.13591363  0.99757573  0.60291202  0.63408613  0.847317    0.75472992
 -1.47172298  1.09172136 -0.85572313 -0.28461408 -0.63719288  0.26592034
 -0.90747196  0.10287975 -0.46635898  0.04209029  0.90000088  0.63595654
  0.93678642 -0.54117668 -0.14900661  0.24970971  0.23817532  0.06453571
 -0.3572496  -0.71232234  0.66307801 -0.28835491 -0.69486494 -0.44734267
 -0.45295399 -0.94394573  2.56282601  1.643191    0.75566507  0.8498106
  0.04894866  0.16429285  0.14839397  0.62878657 -1.45395382 -1.45333019
 -0.56175172 -0.19327399 -0.58357336 -1.54903494 -0.84075945 -0.56299869
  0.86290374  0.85292811  0.79681468 -0.34197432  1.3548306  -0.30550065
  0.00655187  0.76782297  0.84482289  1.03373792  0.29522378  1.18212649
  0.64406172  0.40838594  0.69861655  0.94520328  1.98548216 -0.01776393
  0.10506207  1.35950685 -0.31391772 -0.06670719  1.47204526  1.1253892
  1.2204705   0.30925213 -0.51748451  0.23630496 -0.45513629  0.29958833
 -0.27713235 -0.17581645 -0.37377184 -0.19420927 -0.48288128  0.29927647
  0.20450732 -0.12188535 -0.00654123 -0.47539946 -0.19171531  0.34073803
  0.85853929  0.89251909  0.9925879   0.1483939   1.35296032  0.86664467
 -0.82673112 -0.55738727 -0.81675543 -0.79181618 -0.35880832 -0.44671931
  1.38413422 -0.65620933  0.97544224 -0.77560581  0.00374619 -0.16085299
 -0.55302283 -0.80054504 -0.93459362  0.04333726 -0.20044399 -0.55894595
 -0.38187717 -0.41866256 -0.82610784 -0.20886102 -0.39808768 -0.68488919
 -0.28087314 -0.45482447  0.71482693 -0.22039541 -0.16895814  0.29865301]
Pearson r: 0.767
MSE: 0.473
Fold 3, k_feat 4096
X_train.shape:  (265, 4096)
y_pred:  [ 1.21596819e+00  1.49230938e+00  1.58805647e+00  1.59453760e+00
  6.71535556e-01  1.28490608e+00  1.36621763e+00  3.73393817e-01
 -1.09521971e+00  3.64555603e-01  6.33236746e-01  1.08987671e+00
 -9.65003257e-01  6.75923553e-02  1.34467860e-01  6.75923553e-02
  1.35057064e-01 -7.59662750e-01  6.40896593e-01  6.92452620e-01
 -2.51171946e-01 -8.48339194e-01 -4.11437945e-01 -2.16408315e-01
  1.13256303e-01 -8.46277044e-01 -1.23280090e+00 -5.43716193e-01
 -4.54745129e-01 -6.35386490e-03  9.10166854e-01  7.17788841e-01
  6.13203452e-01 -4.95990020e-01 -2.63839925e-01 -7.90301792e-01
 -1.96113691e-02 -7.57011202e-01 -6.01459092e-01  5.92286428e-01
 -4.38541624e-01  6.64170613e-01  6.53859386e-01  4.78537254e-02
  7.53436190e-01 -5.74944433e-01  1.11197218e+00  9.31378403e-01
  6.21747284e-01  7.61390567e-01  3.98992691e-02  5.08029028e-01
  6.83909106e-01  2.73816835e-01  6.60929780e-01  2.99447595e-01
 -1.62495362e-01  6.28228456e-01  6.09079034e-01 -5.32521139e-01
  1.42808497e+00 -3.69898452e-01  2.53639440e+00  4.66194907e-01
  3.00331491e-01  3.20364718e-01 -2.11989202e-01  6.22894836e-02
 -3.00076576e-01 -2.61483175e-01  8.69805882e-01  4.32904329e-01
  5.42203387e-01 -2.91238378e-01 -9.12858175e-01  2.33455690e-01
  9.94097936e-02 -2.82400311e-01  2.04878861e-01  1.15318515e-01
  2.03995021e-01  1.82783328e-01  2.51132202e-01 -4.77724441e-01
  4.37323438e-01 -4.73042423e-02  4.27895970e-01  1.80983652e-02
  1.35646310e-01 -6.99889115e-02 -3.66952354e-01  1.43011474e-01
 -3.36313260e-01  5.81649715e-02 -8.37438898e-01 -8.68961780e-01
 -6.77467464e-01 -8.34787423e-01  1.32111124e-01  1.17380783e-01
  2.41704760e-01  7.56971494e-01  7.40768188e-01  2.52015952e-01
  9.03096263e-01 -7.11052617e-01  6.93336423e-01 -5.05122816e-01
  1.08248056e-01  3.61609597e-01 -4.29703437e-01 -8.13575741e-01
 -7.79432405e-02 -7.26666771e-01 -5.68757736e-01 -5.09541826e-01
 -3.15985293e-01 -1.49532670e-01 -3.89931565e-01 -7.83525870e-01
 -5.35172587e-01 -1.46586609e-01  7.32813712e-01 -2.75918883e-01
 -9.59700478e-01 -5.50492154e-01 -4.75956750e-01  2.70870751e-01
 -1.34560951e-03 -4.96284493e-01  3.48911080e-02 -5.46956891e-01]
Pearson r: 0.743
MSE: 0.359
Average pearson_r accross all folds = 0.7547021361699077
Average mse accross all folds = 0.43960418990355743
Fold 1, k_feat 8192
X_train.shape:  (264, 8192)
y_pred:  [ 1.04473406  1.69186567  1.51034563  1.50226423  1.16191383  1.43046418
  1.34405566  1.43139664  1.37607034  0.86601136  0.46038867  0.97013681
 -0.91872887 -0.76238528 -1.28612031  0.55736527  0.83182099  0.64501677
  0.08864553 -0.4894835  -0.01920972  1.27349906 -0.44752256 -0.41208883
 -0.96566292 -0.89821463  0.72800641 -0.16778264 -0.89728205  0.36061449
 -0.02045292 -0.42762989  0.95677139  0.43738783  0.91978352 -0.03816994
 -0.02604783 -0.61598791 -1.27648482 -0.11680788 -0.68343645 -0.35676263
 -0.97125774 -0.97902828 -0.86215924  0.44764501  0.50141703 -0.54760746
 -0.71265373 -1.02440827  0.13464725 -0.20974374 -0.56004014 -0.61412319
  0.59590711  0.62947588 -0.53455272 -1.4819383  -0.78631861  0.68386963
  0.19246025 -1.41480081 -0.33096431  0.86103828  0.18033799  0.36993944
  0.21670424 -0.06241399 -0.03630495 -0.22124412  0.27327377 -0.58956837
  0.81161755  0.63475978  0.33637037 -0.12582184 -0.88298424 -0.44161696
  0.23970496  0.09268615 -0.01330415  0.08118578  0.14148522  0.07496932
  0.04450878 -0.55133714  0.0612931   0.0914428  -0.19047284  0.07932083
 -0.56625667  0.13837692  0.10760561 -0.44565765  0.63600295  1.03665269
 -1.01228624 -0.86619979  1.06245092  0.87098452  1.06711331 -0.67038172
  0.73297951 -0.06365725 -0.49383495  0.23597515  0.23784006 -0.49414585
  0.03487321  0.50763359 -0.17803987 -0.209122   -0.26165097  0.0236836
 -0.37727676 -0.00926348 -0.36080315  0.25338116 -0.682504   -0.25729947
 -0.40058839 -0.51248433 -0.94048639 -0.3213288  -0.52864707 -0.91903962
 -0.56159422 -0.29335484  0.18686522 -0.44099528 -0.4161295  -0.50533542
 -0.12302437]
Pearson r: 0.766
MSE: 0.472
Fold 2, k_feat 8192
X_train.shape:  (265, 8192)
y_pred:  [ 1.11603728  1.37447045  1.091098    1.36667679  1.38569313  1.15936926
 -0.2054318   1.23138141  0.67367711  0.58576624  0.90467729  0.59293642
 -1.42963797  0.90124774 -0.88377984 -0.15804728 -0.63126981  0.29958834
 -0.81644372  0.16242222 -0.41305125  0.06796486  0.92494008  0.71576216
  0.8460698  -0.51031434 -0.13466654  0.14558829  0.13623607  0.07388787
 -0.41492164 -0.80210381  0.50066101 -0.25188125 -0.7774762  -0.39060588
 -0.44266663 -1.00535865  2.60085838  1.51101286  0.81458408  0.69206979
  0.04115517  0.11690821  0.14652352  0.59168938 -1.41654497 -1.42870267
 -0.49628608 -0.14433055 -0.64093372 -1.71862192 -0.75908345 -0.50002697
  1.04651909  0.78434513  0.79432075 -0.13871925  1.24821526 -0.16272334
  0.2840013   0.9280577   0.81396044  0.86602133  0.21915899  0.96951926
  0.61039382  0.42272602  0.78777434  1.2463449   1.85673325  0.06297692
  0.12345469  0.98323569 -0.11378011 -0.13373148  1.30495218  0.99352285
  1.1431587   0.3553898  -0.53743591  0.25812683 -0.37283671  0.1013212
 -0.27214451 -0.15243579 -0.31111193 -0.14588936 -0.33137522  0.29865292
  0.16491616 -0.13778412 -0.06639547 -0.4436019  -0.16459379  0.29709426
  0.77530453  0.82206568  0.96702515  0.12688382  1.49854329  0.85760422
 -0.80865015 -0.48412817 -0.85104697 -0.75035469 -0.28523743 -0.47695815
  1.2516444  -0.66275589  0.9988228  -0.74006729 -0.09226993  0.04894858
 -0.6200471  -0.81550856 -0.87723332  0.11597292 -0.08478824 -0.3753306
 -0.18142781 -0.4928569  -0.93584064 -0.20231447 -0.22943592 -0.52777183
 -0.12562624 -0.61194186  0.84544637  0.04209032 -0.25406343  0.41836147]
Pearson r: 0.764
MSE: 0.480
Fold 3, k_feat 8192
X_train.shape:  (265, 8192)
y_pred:  [ 1.20978148  1.63607729  1.48464948  1.6328365   0.63176374  1.27047045
  1.43986921  0.28265507 -1.01007839  0.33980869  0.72809998  1.25868622
 -0.89694916 -0.03876063  0.12386204 -0.03964446  0.12769191 -0.78146367
  0.68243607  0.44940231 -0.36577382 -0.84274174 -0.44384466 -0.32040442
  0.06258396 -0.77940136 -1.0088999  -0.57818517 -0.39906447  0.1612772
  0.95818776  0.76580957  0.77081796 -0.49157096 -0.2611884  -0.79265866
 -0.0661592  -0.74846771 -0.60705662  0.81294673 -0.52986965  0.57608331
  0.57785095  0.0180985   0.84446955 -0.61707317  1.00414624  0.94404649
  0.45647304  0.82797167 -0.02815483  0.43378819  0.73192992  0.15273363
  0.66210819  0.34570081 -0.27532961  0.6229255   0.61114123 -0.44826376
  1.34618436 -0.39641306  2.58382606  0.42877979  0.16275025  0.07701979
 -0.26737525  0.0855633  -0.31480689 -0.23261173  0.96172304  0.45058073
  0.7425357  -0.35310573 -0.94261337  0.20664648  0.11796995 -0.30479037
  0.21253871  0.13417337  0.19604074  0.16716926  0.17748054 -0.4470853
  0.46590023  0.17630211  0.47444386  0.1889702   0.21548469 -0.15954931
 -0.3398485   0.10736409 -0.34721374 -0.00458616 -0.8766215  -0.8804514
 -0.78529347 -0.77262543  0.17365062 -0.08324628  0.36043119  0.89779337
  0.81412509  0.24376695  1.05216716 -0.63504418  0.73045681 -0.48744649
  0.04726447  0.59906238 -0.32541276 -0.71134725 -0.07175654 -0.60381581
 -0.63975786 -0.36783605 -0.16396836 -0.03286853 -0.39493989 -0.71723943
 -0.435301   -0.19519668  0.51215348 -0.16337918 -0.9051983  -0.47389441
 -0.3360186   0.24406153  0.04520223 -0.54636766  0.01073333 -0.46387781]
Pearson r: 0.736
MSE: 0.369
Average pearson_r accross all folds = 0.7555260024153615
Average mse accross all folds = 0.4402492176599555
Fold 1, k_feat 16384
X_train.shape:  (264, 16384)
y_pred:  [ 0.91698623  1.68782509  1.58898355  1.56132044  1.12896665  1.34996128
  1.39565211  1.29587826  1.33566349  0.90330999  0.42340085  1.05841035
 -0.91157996 -0.85905093 -1.24851083  0.48183551  0.76250774  0.68759941
 -0.0073985  -0.47052342  0.06377977  1.32540632 -0.47860477 -0.43042737
 -0.98617716 -0.82113069  0.73360121 -0.09567188 -0.88174091  0.36900668
 -0.05153516 -0.52180901  0.78364355  0.42340077  0.86290316 -0.01112847
 -0.10592908 -0.58739231 -1.29264756  0.00783174 -0.69369356 -0.30081467
 -0.83418519 -0.96006812 -0.85936181  0.49551149  0.27109789 -0.47363173
 -0.74746582 -0.98679877  0.21328517 -0.1879862  -0.50844364 -0.51528179
  0.60492095  0.64097639 -0.58645997 -1.51985861 -0.83480685  0.6546524
  0.32922198 -1.37688049 -0.35085687  0.93967623  0.1324714   0.28943651
  0.18624365 -0.21751421 -0.06676552 -0.13297068  0.22354225 -0.46244202
  0.7914141   0.72396574  0.36403352 -0.1948244  -0.89728205 -0.38597983
  0.2337993   0.18189207 -0.00429036  0.09983508  0.1514315   0.04575202
  0.01498065 -0.51745754  0.09424021  0.18997337 -0.14447119  0.09921344
 -0.55289128  0.15795879  0.1822029  -0.54108009  0.6754774   0.9179187
 -1.05176062 -0.87334873  0.98971857  0.76250764  1.15569766 -0.58583812
  0.74789905 -0.08510406 -0.57154053  0.2720305   0.31336985 -0.50471377
  0.12687663  0.4532398   0.02212963 -0.27470549 -0.35924906 -0.06583302
 -0.315734   -0.04811622 -0.19886491  0.39604851 -0.57651371 -0.33562664
 -0.29957126 -0.52460642 -0.90878255 -0.33251838 -0.33935648 -0.80279214
 -0.4922809  -0.19047272  0.09734846 -0.38815548 -0.13452475 -0.37976329
 -0.11898368]
Pearson r: 0.758
MSE: 0.488
Fold 2, k_feat 16384
X_train.shape:  (265, 16384)
y_pred:  [ 1.26754330e+00  1.35233691e+00  1.11167293e+00  1.40876177e+00
  1.48451499e+00  1.24166886e+00 -7.73064363e-02  1.03966070e+00
  5.58644790e-01  5.01596306e-01  8.56980967e-01  6.77106444e-01
 -1.44990121e+00  9.04988669e-01 -9.34593558e-01 -1.79869154e-01
 -6.61196921e-01  7.41997649e-02 -8.87520604e-01  1.12855515e-01
 -3.54444026e-01  1.93332334e-02  8.39834864e-01  7.65017263e-01
  8.48251965e-01 -4.51083593e-01 -1.10039013e-01  9.13454387e-02
  2.39110503e-01 -5.60597121e-03 -4.56694956e-01 -9.06536922e-01
  3.45725794e-01 -3.04877128e-01 -7.11387185e-01 -3.03941919e-01
 -3.93723326e-01 -1.05835457e+00  2.66788267e+00  1.54125173e+00
  6.93940412e-01  6.31592079e-01  3.64790243e-02  5.45784831e-06
  1.95155099e-01  5.92624677e-01 -1.44990129e+00 -1.47858127e+00
 -5.26836678e-01 -6.54601893e-02 -6.48415454e-01 -1.64037504e+00
 -8.02727108e-01 -5.34941933e-01  9.07170949e-01  5.34328926e-01
  7.63770104e-01 -1.42460198e-01  1.28468902e+00 -2.00755761e-01
  1.30624698e-01  7.67199399e-01  5.35887681e-01  6.52790570e-01
  3.02082024e-01  8.81920041e-01  6.45932310e-01  4.09009409e-01
  6.92069973e-01  1.10138540e+00  1.91814618e+00  1.25325057e-01
  2.74385916e-02  9.81053588e-01 -1.66464310e-01 -2.61856954e-01
  9.70454340e-01  9.71077502e-01  1.10294428e+00  2.29134814e-01
 -4.89116118e-01  2.64673367e-01 -3.66913620e-01  2.52564014e-02
 -1.12533205e-01 -1.24642650e-02 -3.20464181e-01 -1.71763926e-01
 -3.98399525e-01  3.21410041e-01  1.75515366e-01 -2.95213230e-01
 -8.10472785e-02 -4.08063425e-01 -1.79245666e-01  3.17980920e-01
  7.46000890e-01  7.15450348e-01  9.99757920e-01  2.04507265e-01
  1.46238133e+00  8.58851179e-01 -8.37953803e-01 -6.05707074e-01
 -8.45435558e-01 -8.06779764e-01 -3.04253625e-01 -4.66670689e-01
  1.25632059e+00 -7.15128355e-01  1.05462438e+00 -6.56520791e-01
  1.96449926e-02 -9.19581848e-02 -5.69545139e-01 -8.14885079e-01
 -8.69751588e-01  1.72709715e-01 -1.46465346e-02 -4.95662557e-01
 -2.44711236e-01 -5.43047174e-01 -9.84472185e-01 -2.32241601e-01
 -3.15788156e-01 -6.63067375e-01 -3.93723354e-01 -3.82500632e-01
  6.62766229e-01  5.51834100e-02 -1.20950111e-01  5.00037482e-01]
Pearson r: 0.773
MSE: 0.463
Fold 3, k_feat 16384
X_train.shape:  (265, 16384)
y_pred:  [ 1.24542883  1.57803984  1.38094805  1.55417658  0.34216552  1.32556188
  1.2978689   0.40756817 -1.04071747  0.26998691  0.64030723  1.2878523
 -0.87986201 -0.08766532  0.10206112 -0.09503048  0.09528518 -0.80238068
  0.65562691  0.56135281 -0.03021701 -0.76879549 -0.39464539 -0.19755345
  0.07289523 -0.73255891 -1.06222365 -0.54224319 -0.50364981  0.0463807
  0.98146165  0.67536547  0.55251455 -0.54990297 -0.32217201 -0.77203624
 -0.07205136 -0.64240929 -0.60175369  0.58256449 -0.56316015  0.62351494
  0.61939045  0.05079987  0.84005048 -0.63799018  0.94669798  1.01298442
  0.59140281  0.86597591  0.02310677  0.40550594  0.66534891  0.1789536
  0.64502107  0.25997027 -0.25264486  0.54485482  0.5533984  -0.41939228
  1.24071522 -0.37696901  2.48395448  0.33214891  0.07495756  0.10706966
 -0.27238354 -0.0667483  -0.39052077 -0.27032137  0.85419171  0.59611635
  0.56312043 -0.25205578 -0.90755518  0.18160495  0.14271693 -0.21758688
  0.14713611  0.15685805  0.13623566  0.17836431  0.17689129 -0.43147124
  0.42141462  0.06906532  0.51509953  0.14713601  0.1804266  -0.21140006
 -0.29948744  0.14684142 -0.42263301 -0.0411174  -0.82064626 -0.83979571
 -0.78205278 -0.8070944   0.14890372  0.01485772  0.34275482  0.85772694
  0.69657715  0.20370055  0.93078933 -0.67422685  0.67948995 -0.55785735
 -0.01430821  0.61143584 -0.23761998 -0.5719984  -0.15719237 -0.4385416
 -0.61294867 -0.43795241 -0.26383998 -0.12125048 -0.22377349 -0.55108137
 -0.46741307 -0.17398499  0.44851839 -0.30066585 -0.78146352 -0.34986513
 -0.35752488  0.18572945  0.0275258  -0.39169926 -0.096798   -0.59173705]
Pearson r: 0.732
MSE: 0.371
Average pearson_r accross all folds = 0.7543268005086711
Average mse accross all folds = 0.44083283383545546
